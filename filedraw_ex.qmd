---
title: "Filedraw Bias"
subtitle: "Core Empirical Research Methods - Summer Work"
author: "Toby Wang"
date: "Aug 31, 2023"
format: html
geometry:
  - top=30mm
  - left=20mm
  - heightrounded
editor: visual
---

```{r, include = FALSE}
library(tidyverse)
library(janitor)
library(broom)
library(car)
library(ggplot2)
library(modelsummary)
library(gapminder)
library(gridExtra)
library(knitr)
```

## Q1: Patterns in `filedrawer.csv`

### a.

```{r}
filedrawer <- read_csv("https://ditraglia.com/data/filedrawer.csv")
```

### b.

```{r}
datasummary_crosstab(IV ~ DV, data = filedrawer)
```

### c.

```{r}
success <- filedrawer |>
  select(IV, DV) |>
  group_by(IV) |>
  summarize(unpublished = sum(as.numeric(DV == "Unpublished")), 
            published_or_unwritten = sum(as.numeric(DV != "Unpublished"))) |>
  summarize(IV = IV, publication_rate = published_or_unwritten / (published_or_unwritten + unpublished)) 

sample_sizes <- c(sum(as.numeric(filedrawer$IV == "Null")),
sum(as.numeric(filedrawer$IV == "Weak")),
sum(as.numeric(filedrawer$IV == "Strong")))

result <- prop.test(success$publication_rate, sample_sizes)
tidy(result) |>
  knitr::kable()
```

As we can see, the p value of these three values equally each other is .8, which could easily be rejected at any conventional significance level.

```{r}
filedrawer <- filedrawer |>
  mutate(DV_bin = ifelse(filedrawer$DV %in% c("Published, non top", "Published, top"), 1, 0), 
         null = ifelse(filedrawer$IV == "Null", 1, 0), 
         weak = ifelse(filedrawer$IV == "Weak", 1, 0)) 

lreg_pub <- glm(DV_bin ~ null + weak, family = binomial(link = 'logit'), filedrawer)
tidy(lreg_pub) |>
  mutate(odds_of_publish = exp(estimate)) |>
  select(term, odds_of_publish, estimate, statistic) |>
  knitr::kable()
```

Through this we can see that the odds of publishing for a result that has no findings (`null`) is 0.16 times of that of the odds of the strong results. Additionally, we see that the odds of publishing for a result that has weak findings (`weak`) is 0.16 times of that of the odds of strong results.

### d.

We just have to add this in as a control to our previous regression:

```{r}
lreg_pub2 <- glm(DV_bin ~ null + weak + max.h, family = binomial(link = 'logit'), filedrawer)
lreg_pubs <- list(lreg_pub, lreg_pub2)
modelsummary(lreg_pubs, fmt = 3, gof_omit = "Num.Obs.|AIC|BIC|Log.Lik|F")
```

### e.

These two regressions seems to suggest that there is a statistically significant relationship between `max.h` and the rate of publish. However, controlling for the H index did not weaken the other factors, as `weak` is still statistically insignificant and the effect of `null` compared to `strong` is even stronger.

## Q2: Patterns in `published.csv`

### a.

```{r}
published <- read_csv("https://ditraglia.com/data/published.csv")
```

### b.

```{r}
published |>
  ggplot(aes(x = cond.s, y = cond.p))+
  geom_jitter(width = 0.1, height = 0.1) +
  geom_abline(a=0, b=1, col="red")
```

### c.

```{r}
published |>
  ggplot(aes(x = out.s, y = out.p))+
  geom_jitter(width = 0.1, height = 0.1)+
  geom_abline(a=0, b=1, col="red")
```

### d.

Both plots suggests that there is a systematic, downward bias wuch that both have all of the points either on or below the 45 degree line. The points on the 45 degree line imply truthfulness, that all experimental conditions in the study are published and all variables inthe study are published. It is impossible to publish more points than those included in the study so natrually there are no points above this line (in red). Furthermore, we see many points especially in the second plot to be below the red line, indicating that there are conditions and variables which was studied but not published. This may be because journal editors and referees are more willing to accept a paper if it has significant results, so either researchers or editors would exclude the conditions and variables which are weak to have a better chance of publication.

## Q3

### a. b. c.

If we want to find the probability of rejecting at least one null, we first have to find the probability of rejecting no null (accepting every null), and use one to subtract that value. Similarly, if we want to find the probability of rejecting at least three nulls, we have to find the probability of rejecting at most two nulls, which means the probability of accepting `n-2` nulls.

```{r}
outcomes <- length(published$out.s)
conditions <- length(published$cond.s)
alpha <- 0.05
total_hypothesis <- outcomes * conditions

p_one <- 1 - (0.95^total_hypothesis)
p_two <- 1 - (0.95^(total_hypothesis - 1))
p_three <- 1 - (0.95^(total_hypothesis - 2))
data.frame(p_one, p_two, p_three)
```

On average we should see all three (at least reject one, two, and three hypothesis) should have a 100% probability. To check let's run a simulation to see how this probability changes when the number of least rejections increase.

```{r}
hypo_test_func <- \(total_hypothesis, n){
p_n <- 1 - (0.95^(total_hypothesis - n + 1))
p_n
}

result <- map_dbl(2400:2809, \(n) hypo_test_func(total_hypothesis, n))
data.frame(reject = c(2400:2809), result = result) |>
  ggplot(aes(x = reject, y = result)) +
  geom_line()
```

### d.

The diagram above shows that we should on average reject more hypothesis than we do not, however the opposite occurs in journals that incentivise behavior to only show significant results. The findings highlight the potential for publication bias and p-hacking, where researchers might selectively report only the significant results from a large number of tests, leading to an overrepresentation of false positives in the published literature. This is why it is important to pre-register studies and hypothesis tests and to report all results, regardless of whether they are statistically significant.
