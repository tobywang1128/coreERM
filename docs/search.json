[
  {
    "objectID": "colonial_origins_ex.html",
    "href": "colonial_origins_ex.html",
    "title": "The Colonial Origins of Comparative Development",
    "section": "",
    "text": "library(haven)\ndta &lt;- read_dta(\"https://ditraglia.com/data/ajr.dta\")\nhead(dta)\n\n# A tibble: 6 × 14\n  longname   shortnam   mort logmort0  risk loggdp latitude neoeuro  asia africa\n  &lt;chr&gt;      &lt;chr&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 Angola     AGO      280        5.63  5.36   7.77    0.137       0     0      1\n2 Argentina  ARG       68.9      4.23  6.39   9.13    0.378       0     0      0\n3 Australia  AUS        8.55     2.15  9.32   9.90    0.300       1     0      0\n4 Burkina F… BFA      280        5.63  4.45   6.85    0.144       0     0      1\n5 Bangladesh BGD       71.4      4.27  5.14   6.88    0.267       0     1      0\n6 Bolivia    BOL       71        4.26  5.64   7.93    0.189       0     0      0\n# ℹ 4 more variables: other &lt;dbl&gt;, rainmin &lt;dbl&gt;, meantemp &lt;dbl&gt;, malaria &lt;dbl&gt;"
  },
  {
    "objectID": "colonial_origins_ex.html#q1",
    "href": "colonial_origins_ex.html#q1",
    "title": "The Colonial Origins of Comparative Development",
    "section": "Q1",
    "text": "Q1\n\na.\nThe key question that AJR try to answer is why some countries are much richer than others today. Specifically, they investigate the role that institutions play in determining the level of development of various countries and argue that the differences in institutions across countries are the fundamental cause of differences in economic development.\n\n\nb.\nAJR’s key theory is that the institutions in a society are a major determinant of economic development. They argue that societies that have “good” institutions - those that provide security of property rights and relatively equal access to economic opportunities - will have better economic performance than societies with “bad” institutions - those that do not provide these things. They also argue that the colonizers established different types of institutions in different colonies, which led to differences in economic development. Specifically, in places where the disease environment was not suitable for European settlement, colonizers set up extractive institutions, which did not provide secure property rights and did not encourage investment. In places where Europeans settled in large numbers, they set up institutions that were more conducive to investment and economic development.\n\n\nc.\nExogeneity: in this context this would mean that settler mortality rates are uncorrelated with any other factors that affect economic development other than through their effect on early institutions which affects later institutions, which includes them directly affecting economic performance (exclusion). This cannot be tested because we do now know what the error term is (as it could mean anything that causes economic development except institutions). AJR argue that this holds because the factors which caused settlers mortaility was determined by the disease (malaria and yellow fever) environment under which the settlers faced, which the locals have a built immunity for (till this day). Therefore these diseases are therefore unlikely to be the reason why many countries in Africa and Asia are very poor today.\nRelavence: The instrument they use is the mortality rates of settlers. The relevance assumption here would mean that settler mortality rates are correlated with the institutions that were established by the colonizers. This can be tested and AJR claims in their paper that mortality rates faced by the settlers more than 100 years ago explains over 25 percent of the variation in current institutions."
  },
  {
    "objectID": "colonial_origins_ex.html#q2-ols-regression",
    "href": "colonial_origins_ex.html#q2-ols-regression",
    "title": "The Colonial Origins of Comparative Development",
    "section": "Q2: OLS Regression",
    "text": "Q2: OLS Regression\n\na.\n\nols &lt;- lm(loggdp ~ risk, dta)\n\n\n\nb.\n\ntidy(ols) |&gt;\n  knitr::kable(digits = 2, caption = \"OLS\")\n\n\nOLS\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n4.73\n0.41\n11.41\n0\n\n\nrisk\n0.51\n0.06\n8.11\n0\n\n\n\n\n\n\n\nc.\nwe cannot interpret these results causally because there may be variables, such as geographical location, which causes both risk and loggdp. The natural resource curse is a good illustration of this whereas it could both cause more extractive institutions, and higher gdp. Additionally, there may be reverse causation in that it may actually be loggdp causing better institutions in line with theories which state that a country would be more democratic and have better property rights once there is a growing middle class which wants their property more secure and protected."
  },
  {
    "objectID": "colonial_origins_ex.html#q3-iv-regression",
    "href": "colonial_origins_ex.html#q3-iv-regression",
    "title": "The Colonial Origins of Comparative Development",
    "section": "Q3: IV Regression",
    "text": "Q3: IV Regression\n\na. First Stage\n\nfirst_stage &lt;- lm(risk ~ logmort0, dta)\nfirst_stage |&gt;\n  tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    9.39      0.633     14.8  1.00e-21\n2 logmort0      -0.620     0.131     -4.74 1.37e- 5\n\n\nThere is a very significant relationship between settlers mortality rates and the risk of expropriation. Specifically, one percent change in settlers mortality rates is correalted with a .6 point reduction (out of 10) in how protected one’s property is.\n\n\nb. Reduced Form\n\nreduced_form &lt;- lm(loggdp ~ logmort0, dta)\nreduced_form |&gt;\n  tidy() |&gt;\n  knitr::kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n10.6336150\n0.3822061\n27.821675\n0\n\n\nlogmort0\n-0.5608809\n0.0789457\n-7.104643\n0\n\n\n\n\n\nAgain there is a statistically significant relationship between logmort0 and loggdp, indicating that a 1 percent increase in settler mortality rates is associated with a 0.56 percent decrease in GDP.\n\n\nc.\n\niv &lt;- AER::ivreg(loggdp ~ risk | logmort0, data = dta)\n\n\n\nd.\n\niv_ols &lt;- list(iv_reg = iv, ols_reg = ols)\nmodelsummary(iv_ols, gof_omit = 'Log.Lik.|R2 Adj.|AIC|BIC|F', fmt = 2)\n\n\n\n\n\niv_reg\nols_reg\n\n\n\n\n(Intercept)\n2.13\n4.73\n\n\n\n(1.01)\n(0.41)\n\n\nrisk\n0.91\n0.51\n\n\n\n(0.16)\n(0.06)\n\n\nNum.Obs.\n62\n62\n\n\nR2\n0.195\n0.523\n\n\nRMSE\n0.92\n0.71\n\n\n\n\n\n\n\nThe IV regression gives a larger effect of a unit increase of risk (better institutions) on the percentage increase of gdp in 1995. This results strengthens AJR’s argument that institutions matter in economic development, when holding other factors equal. They show this argument through this IV result which show that confounding variables (endogenous) bias the true causal result downwards.\n\n\ne.\nSince we know that given that the instrument satisfies relevance, the exclusion restriction, and independence, the IV coefficient is the reduced form coefficient over the first stage coefficient, we have:\n\niv_hand &lt;- coef(reduced_form)[2] / coef(first_stage)[2]\nround(iv_hand, digits = 2)\n\nlogmort0 \n    0.91 \n\n\nThis is the same result we got for the IV coefficient rounded to 2 decimal points."
  },
  {
    "objectID": "colonial_origins_ex.html#q4-2-criticisms-of-ajr",
    "href": "colonial_origins_ex.html#q4-2-criticisms-of-ajr",
    "title": "The Colonial Origins of Comparative Development",
    "section": "Q4: 2 Criticisms of AJR",
    "text": "Q4: 2 Criticisms of AJR\n\na.\nPutting claims 1 and 2 together we can argue that the instrument logmort0 violates the exclusion restriction, which is included in the exogeneity assumption. Claim 2 states that settler mortality rates causes a country’s disease environment today, and claim 1 states that the current disease environment determines GDP per capita. Taken together this questions if institutions is the only bridge between settlers moratality and GDP per capita.\n\n\nb.\nThis is because when we include malaria (today) as an additional regressor, we control for the current disease enviornment (or we pull it out from the error term in the causal model) when measuring the effect of institutions on GDP. By doing so claim 1 and 2 cannot hold because the current disease enviornment is no longer a part of the error term. Thus, if after controlling for malaria (current disease enviornment) we do not see a change in the IV results, this would mean that the claims (1 and 2) are invalid, and we do see a change in IV results, our IV estimates would be strengthened.\n\n\nc. OLS with malaria\n\nols_malaria &lt;- lm(loggdp ~ risk + malaria, dta)\nols_malaria |&gt;\n  tidy() |&gt;\n  knitr::kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n6.2958652\n0.4087284\n15.403542\n0e+00\n\n\nrisk\n0.3388912\n0.0554135\n6.115683\n1e-07\n\n\nmalaria\n-1.1454612\n0.1825618\n-6.274375\n0e+00\n\n\n\n\n\nWe cannot interpret this causally because the error term of a causal model represents all other causes of loggdp, some of which may also cause institutions or malaria (for instance, geography). Additionally, GDP may also cause good or bad institutions. Simply controlling for malaria would not work for a causal interpretation. Also note that the magnitude on risk on GDP is smaller when controlling for malaria, indicating that malaria may have previously been a factor which explains both risk and loggdp.\n\n\nd. First stage with malaria\n\nfirst_stage_malaria &lt;- lm(risk ~ logmort0 + malaria, dta)\nfirst_stage_malaria |&gt;\n  tidy() |&gt;\n  knitr::kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n8.8342124\n0.7539641\n11.717020\n0.0000000\n\n\nlogmort0\n-0.4380426\n0.1881859\n-2.327712\n0.0233768\n\n\nmalaria\n-0.6962385\n0.5220329\n-1.333706\n0.1874262\n\n\n\n\n\nThis would seem to suggest that a 1 percent increase in mortality rate is associated with .43 point decrease in risk protection. Notice that the p value for malaria is 0.187, which does not pass the 95% test. Intuitively this makes sense if we want to make the case that malaria represents a separate causal path from settler mortality to GDP.\n\n\ne. IV with malaria\n\niv_malaria &lt;- AER::ivreg(loggdp ~ risk + malaria | malaria + logmort0, data = dta)\n\nreg_malaria &lt;- list(ols_malaria = ols_malaria, iv_malaria = iv_malaria, ols = ols, iv = iv)\nmodelsummary(reg_malaria, fmt = 3, gof_omit = 'Log.Lik.|R2 Adj.|AIC|BIC|F')\n\n\n\n\n\nols_malaria\niv_malaria\nols\niv\n\n\n\n\n(Intercept)\n6.296\n4.505\n4.731\n2.135\n\n\n\n(0.409)\n(1.591)\n(0.415)\n(1.014)\n\n\nrisk\n0.339\n0.589\n0.505\n0.905\n\n\n\n(0.055)\n(0.222)\n(0.062)\n(0.155)\n\n\nmalaria\n−1.145\n−0.751\n\n\n\n\n\n(0.183)\n(0.396)\n\n\n\n\nNum.Obs.\n62\n62\n62\n62\n\n\nR2\n0.714\n0.615\n0.523\n0.195\n\n\nRMSE\n0.55\n0.64\n0.71\n0.92\n\n\n\n\n\n\n\nComparing ols_malaria and iv_malaria, we see that the IV estimate is indeed larger than the OLS estimate in magnitude in the effect of good institutions on good economic performance.\n\n\nf.\nThis criticism is valid in that it shows that, controlling for malaria, both the OLS and IV estimates for the effect of institutions have gone down. Therefore it shows that AJR’s estimates are likely overblown. However, in both AJR’s analysis and when we control for malaria, the iv estimates are consistently higher than the OLS estimates, which suggests that the fundamental logic of AJR’s paper, that institutions matter, is still sound, with a caveat that controlling for malaria, the magnitude of this effect goes back to the original OLS level."
  },
  {
    "objectID": "colonial_origins_ex.html#q5-jeffery-sachs-criticism-of-ajr",
    "href": "colonial_origins_ex.html#q5-jeffery-sachs-criticism-of-ajr",
    "title": "The Colonial Origins of Comparative Development",
    "section": "Q5: Jeffery Sachs Criticism of AJR",
    "text": "Q5: Jeffery Sachs Criticism of AJR\n\na.\n\niv_geography &lt;- AER::ivreg(loggdp ~ risk + malaria + latitude + rainmin + meantemp | malaria + latitude + rainmin + meantemp + logmort0, data = dta)\n\nols_geography &lt;- lm(loggdp ~ risk + malaria + latitude + rainmin + meantemp, dta)\n\nreg_malaria_geo &lt;- list(iv_geography = iv_geography, ols_geography = ols_geography, ols_malaria = ols_malaria, iv_malaria = iv_malaria)\n\nmodelsummary(reg_malaria_geo, fmt = 3, gof_omit = 'Log.Lik.|AIC|BIC|F')\n\n\n\n\n\niv_geography\nols_geography\nols_malaria\niv_malaria\n\n\n\n\n(Intercept)\n3.826\n6.956\n6.296\n4.505\n\n\n\n(9.663)\n(0.648)\n(0.409)\n(1.591)\n\n\nrisk\n0.680\n0.258\n0.339\n0.589\n\n\n\n(1.298)\n(0.062)\n(0.055)\n(0.222)\n\n\nmalaria\n−0.746\n−0.998\n−1.145\n−0.751\n\n\n\n(0.818)\n(0.198)\n(0.183)\n(0.396)\n\n\nlatitude\n−0.444\n0.317\n\n\n\n\n\n(2.510)\n(0.678)\n\n\n\n\nrainmin\n−0.001\n0.007\n\n\n\n\n\n(0.025)\n(0.003)\n\n\n\n\nmeantemp\n0.009\n−0.018\n\n\n\n\n\n(0.086)\n(0.017)\n\n\n\n\nNum.Obs.\n62\n62\n62\n62\n\n\nR2\n0.537\n0.745\n0.714\n0.615\n\n\nR2 Adj.\n0.495\n0.722\n0.704\n0.602\n\n\nRMSE\n0.70\n0.52\n0.55\n0.64\n\n\n\n\n\n\n\n\n\nb.\nAfter controlling for the three geographical factors in our IV analysis, we find that nothing in the regression has significant results. However, if we discard settlers mortality as an IV and just run an OLS, none of the geographical explanations are while institutions and malaria are. This may be due to overfitting, whereas the more variables we add the larger that standard deviation would be, or it may say something more important about the instrument. Let’s investigate further.\nOn first sight, there may seem to be a connection between the settlers mortality rates and geography. This would be problematic because this would violate the independence assumption for the instrument logmort0, as Sachs is also claiming that geography is also a factor which GDP depends on. Let’s first run some quick regressions\nTesting if mortality rates and geography are related\n\ngeo_mort &lt;- lm(logmort0 ~ latitude + meantemp + rainmin, dta)\ntidy(geo_mort)|&gt;\n  knitr::kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n3.4115397\n0.7701581\n4.429662\n0.0000424\n\n\nlatitude\n-2.1375517\n1.0764409\n-1.985758\n0.0517940\n\n\nmeantemp\n0.0890122\n0.0275084\n3.235824\n0.0020058\n\n\nrainmin\n-0.0163914\n0.0038366\n-4.272360\n0.0000728\n\n\n\n\n\nTesting if gdp and geography are related\n\ntidy(lm(loggdp ~ latitude + meantemp + rainmin, dta)) |&gt;\n  knitr::kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n8.6603721\n0.6177885\n14.018345\n0.0000000\n\n\nlatitude\n2.0757470\n0.8634757\n2.403944\n0.0194351\n\n\nmeantemp\n-0.0610336\n0.0220661\n-2.765950\n0.0076005\n\n\nrainmin\n0.0158950\n0.0030776\n5.164785\n0.0000031\n\n\n\n\n\nIt seems as though latitude is quite weak, but rainmin and meantemp seems to have a strong correlation with both the instrument and the outcome variables. Stopping to think for a second, as someone who’s experienced both the northern, dry parts of China and southern, humid parts of China, both temperature and rainfall are causes of mosquito infestations, which are the primary medium for malaria, which we use as a proxy for disease today. If we can prove that, controlling for disease, geography itself does not explain settlers mortality, then we can say that we have actually controlled for geography in the malaria control, and would not have to proxy for all the extra variables which are making our standard errors too big. Let’s test for this\n\ngeo_mort_malaria &lt;- lm(logmort0 ~ latitude + meantemp + rainmin + malaria, dta)\n\ngeo_mort_regs &lt;- list(without_malaria = geo_mort, with_malaria = geo_mort_malaria)\n\nmodelsummary(geo_mort_regs, fmt = 3, gof_omit = 'Log.Lik.|AIC|BIC|F')\n\n\n\n\n\nwithout_malaria\nwith_malaria\n\n\n\n\n(Intercept)\n3.412\n3.151\n\n\n\n(0.770)\n(0.648)\n\n\nlatitude\n−2.138\n−0.565\n\n\n\n(1.076)\n(0.955)\n\n\nmeantemp\n0.089\n0.057\n\n\n\n(0.028)\n(0.024)\n\n\nrainmin\n−0.016\n−0.011\n\n\n\n(0.004)\n(0.003)\n\n\nmalaria\n\n1.401\n\n\n\n\n(0.277)\n\n\nNum.Obs.\n62\n62\n\n\nR2\n0.471\n0.635\n\n\nR2 Adj.\n0.444\n0.609\n\n\nRMSE\n0.90\n0.75\n\n\n\n\n\n\n\nAs we can see, when we control for malaria, even though the three geography controls are still significant to some extent, none of them have any significant impact on explaining settlers mortality compared to before (especially with the latitude factor). We also see a jump in adj. R squared. Even though these results does not completely prove my point (meantemp and rainmin still have very small yet significant impact on settlers mortality controlling for malaria), it does call in to question the need to control for three new variables in the IV regression, which increases the standard error of each coefficient estiamte significantly and does not contribute massively to explaining the variance if most is already captured in the malaria variable (which we also controlled for in the IV regression)."
  },
  {
    "objectID": "contaminated_wells_ex.html",
    "href": "contaminated_wells_ex.html",
    "title": "Comtaminated Wells in Bangladesh",
    "section": "",
    "text": "data_url &lt;- 'https://ditraglia.com/data/wells.csv'\nwells &lt;- read_csv(data_url)\n\nRows: 3020 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (5): switch, arsenic, dist, assoc, educ\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(wells)\n\n# A tibble: 6 × 5\n  switch arsenic  dist assoc  educ\n   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1      1    2.36  16.8     0     0\n2      1    0.71  47.3     0     0\n3      0    2.07  21.0     0    10\n4      1    1.15  21.5     0    12\n5      1    1.1   40.9     1    14\n6      1    3.9   69.5     1     9\n\n\n\n\n\n\nwells &lt;- wells |&gt;\n  mutate(larsenic = log(arsenic))\n\n\n\n\n\np1 &lt;- wells |&gt;\n  mutate(larsenic = log(arsenic)) |&gt;\n  ggplot(aes(x = arsenic)) +\n  geom_histogram()\n\np2 &lt;- wells |&gt;\n  mutate(larsenic = log(arsenic)) |&gt;\n  ggplot(aes(x = larsenic)) +\n  geom_histogram()\n\ngrid.arrange(p1, p2, ncol = 2)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nMost households have arsenic levels close to 0, and very few have levels greater than 5. Therefore when we take the log of these values most will fall between negative infinity (those that are close to 0) to 1 (approx.), below is a natural log plot to better visualize this relationship\n\nx &lt;- seq(0, 10, by = 0.01)\ny &lt;- log(x)\ndata.frame(x,y) |&gt;\n  ggplot(aes(x = x, y = y)) +\n  geom_line() +\n  geom_abline(intercept = log(2.5), slope = 0, color = \"blue\")\n\n\n\n\n\n\n\n\nwells &lt;- wells |&gt;\n  mutate(dist100 = dist/100)\n\n\n\n\n\nwells &lt;- wells |&gt;\n  mutate(zeduc = (educ - mean(educ)) / sd(educ) )"
  },
  {
    "objectID": "contaminated_wells_ex.html#q1-preliminarites",
    "href": "contaminated_wells_ex.html#q1-preliminarites",
    "title": "Comtaminated Wells in Bangladesh",
    "section": "",
    "text": "data_url &lt;- 'https://ditraglia.com/data/wells.csv'\nwells &lt;- read_csv(data_url)\n\nRows: 3020 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (5): switch, arsenic, dist, assoc, educ\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(wells)\n\n# A tibble: 6 × 5\n  switch arsenic  dist assoc  educ\n   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1      1    2.36  16.8     0     0\n2      1    0.71  47.3     0     0\n3      0    2.07  21.0     0    10\n4      1    1.15  21.5     0    12\n5      1    1.1   40.9     1    14\n6      1    3.9   69.5     1     9\n\n\n\n\n\n\nwells &lt;- wells |&gt;\n  mutate(larsenic = log(arsenic))\n\n\n\n\n\np1 &lt;- wells |&gt;\n  mutate(larsenic = log(arsenic)) |&gt;\n  ggplot(aes(x = arsenic)) +\n  geom_histogram()\n\np2 &lt;- wells |&gt;\n  mutate(larsenic = log(arsenic)) |&gt;\n  ggplot(aes(x = larsenic)) +\n  geom_histogram()\n\ngrid.arrange(p1, p2, ncol = 2)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nMost households have arsenic levels close to 0, and very few have levels greater than 5. Therefore when we take the log of these values most will fall between negative infinity (those that are close to 0) to 1 (approx.), below is a natural log plot to better visualize this relationship\n\nx &lt;- seq(0, 10, by = 0.01)\ny &lt;- log(x)\ndata.frame(x,y) |&gt;\n  ggplot(aes(x = x, y = y)) +\n  geom_line() +\n  geom_abline(intercept = log(2.5), slope = 0, color = \"blue\")\n\n\n\n\n\n\n\n\nwells &lt;- wells |&gt;\n  mutate(dist100 = dist/100)\n\n\n\n\n\nwells &lt;- wells |&gt;\n  mutate(zeduc = (educ - mean(educ)) / sd(educ) )"
  },
  {
    "objectID": "contaminated_wells_ex.html#q2-first-regression-fit1",
    "href": "contaminated_wells_ex.html#q2-first-regression-fit1",
    "title": "Comtaminated Wells in Bangladesh",
    "section": "Q2: First Regression: fit1",
    "text": "Q2: First Regression: fit1\n\na)\n\nfit1 &lt;- glm(switch ~ dist100, family = binomial(link = 'logit'), wells)\nmodelsummary(fit1)\n\n\n\n\n\n (1)\n\n\n\n\n(Intercept)\n0.606\n\n\n\n(0.060)\n\n\ndist100\n−0.622\n\n\n\n(0.097)\n\n\nNum.Obs.\n3020\n\n\nAIC\n4080.2\n\n\nBIC\n4092.3\n\n\nLog.Lik.\n−2038.119\n\n\nF\n40.744\n\n\nRMSE\n0.49\n\n\n\n\n\n\n\n\n\nb)\n\nwells |&gt;\n  ggplot(aes(x = dist100, y = switch)) +\n  stat_smooth(method = 'glm', method.args = list(family = \"binomial\"), formula = y ~ x) +\n  geom_jitter(width = 0.1, height = 0.01) + \n  xlab(\"distance to nearest clean well / 100 m\") +\n  ylab(\"predicted probability for switching\")\n\n\n\n\n\n\nc)\nFrom the first two parts we see that there is a small but significant relationship between the distance (a 100m increase in distance to a clean well will decrease the log odds by approx. 0.6). This would make sense since we would assume that, given that the households know if their nearest well is contaminated or not, that they would be less inclined to move if the nearest clean well is far.\n\n\nd)\n\nmean_dist &lt;- wells |&gt;\n  select(dist100) |&gt;\n  summarize(dist100 = mean(dist100))\n\nprob_avg &lt;- predict(fit1, mean_dist, type = 'response')\nprob_avg\n\n        1 \n0.5757602 \n\n\nFor the average household (in terms of distance to the nearest safe well), the probability of switching is approx .57\n\n\ne)\nFirst let us calculate the marginal effect from taking the derivative of \\(P(Y = 1 | X)\\)\n[ \\[\\begin{align}\n\\frac{\\partial p(x)}{\\partial x} &= \\beta_1 \\frac{e^{\\beta_0 + \\beta_1 X}}{(1 + e^{\\beta_0 + \\beta_1 X})^2} \\\\\n&= \\beta_1 \\frac{e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}} \\frac{1}{1 + e^{\\beta_0 + \\beta_1 X}} \\\\\n& = \\beta_1 P(Y = 1) (1-P(Y = 1))\n\\end{align}\\] ]\nNow we calcualte this: by using the result from the previous question prob_avg which represent \\(P(Y = 1)\\):\n\nmarginal_avg &lt;- as.numeric(coef(fit1)[2] * prob_avg * (1 - prob_avg))\nmarginal_avg\n\n[1] -0.1519011\n\n\nThis figure suggests that for the person which corresponds to the average distance towards the nearest clean well, the regression would predict that if they live 100 meters further, we would predict a 0.15 decrease in the probability of moving. Now let’s find the “divide by 4” and average partial effect\n\ndiv4 &lt;- as.numeric(coef(fit1)[2]/4)\n\navg_partial &lt;- mean(coef(fit1)[2] * predict(fit1, type = 'response') * (1 - predict(fit1, type = 'response')))\n\ndata.frame(divide_by_4 = div4, avg_partial, marginal_at_mean = marginal_avg)\n\n  divide_by_4 avg_partial marginal_at_mean\n1  -0.1554705   -0.149842       -0.1519011\n\n\nThe marginal effect of the average household is sandwiched between the average partial effect and the “divide by 4” value. This suggests that the effect of the independent variable on the probability of the event occurring at the average values of the predictors is less than the maximum possible effect (as given by the “divide by 4” rule). Additionally, on average, the effect of the independent variable on the probability of the event occurring is less than its effect at the average values of the predictors."
  },
  {
    "objectID": "contaminated_wells_ex.html#q3-predictive-performance-of-fit1",
    "href": "contaminated_wells_ex.html#q3-predictive-performance-of-fit1",
    "title": "Comtaminated Wells in Bangladesh",
    "section": "Q3: Predictive performance of fit1",
    "text": "Q3: Predictive performance of fit1\n\na)\n\nwells &lt;- wells |&gt;\n  mutate(p1 = predict(fit1, type = 'response')) |&gt;\n  select(p1, dist100, everything())\n\n\n\nb)\n\nwells &lt;- wells |&gt; \n  mutate(pred1 = ifelse(p1 &gt; 0.5, 1, 0)) |&gt;\n  select(pred1, everything())\n\nI am going to assume to use the bayes classifer rule for constructing the pred1 ## c)\n\nerror_fit1 &lt;- wells |&gt; \n  summarize(error_rate = mean(pred1 - switch))\nerror_fit1\n\n# A tibble: 1 × 1\n  error_rate\n       &lt;dbl&gt;\n1      0.317\n\n\nThis means that pred1 overestimates the switch rate for approx. 30 percent of the data.\n\n\nd)\n\ntrue_positive &lt;- sum(as.numeric(wells$pred1 == 1 & wells$switch == 1))\ntrue_negative &lt;- sum(as.numeric(wells$pred1 == 0 & wells$switch == 0))\nfalse_positive &lt;- sum(as.numeric(wells$pred1 == 1 & wells$switch == 0))\n# your website has a typo when explaining what this one was\nfalse_negative &lt;- sum(as.numeric(wells$pred1 == 0 & wells$switch == 1))\nconf_mat &lt;- table(1:2, 1:2)\nconf_mat[1,1] &lt;- true_negative\nconf_mat[2,2] &lt;- true_positive\nconf_mat[1,2] &lt;- false_negative\nconf_mat[2,1] &lt;- false_positive\nconf_mat\n\n   \n       1    2\n  1  194  133\n  2 1089 1604\n\n# I later found out that there is an easier way to do this... \nconf_mat1 &lt;- table(wells$pred1, wells$switch)\n\n\n\ne)\n\nsens_fit1 &lt;- conf_mat1[2,2] / (conf_mat1[2,2] + conf_mat1[1,2])\nspes_fit1 &lt;- conf_mat1[1,1] / (conf_mat1[1,1] + conf_mat1[2,1])\ndata.frame(cbind(sens_fit1, spes_fit1))\n\n  sens_fit1 spes_fit1\n1 0.9234312 0.1512081\n\n\n\n\nf)\nThe results in the previous part makes intuitive sense because we found in part c) that pred1 overestimates the switch rate for approx. 30 percent of the data. This means that pred1 should systematically pick up true values well but false values poorly.\n\nwells |&gt;\n  summarize(mean(switch))\n\n# A tibble: 1 × 1\n  `mean(switch)`\n           &lt;dbl&gt;\n1          0.575\n\n\nThis would mean that the null model predicts a 100 percent switch rate, let’s use this to calculate the error rate (we don’t need to calcualte the sensitivity and specificity because they would be 100 percnet and 0 percent respectively)\n\nwells |&gt;\n  summarize(error_rate_null = 1 - mean(switch))\n\n# A tibble: 1 × 1\n  error_rate_null\n            &lt;dbl&gt;\n1           0.425\n\n\nThe null model overclassifies approx. 42 percent of the data, which means that the null model performs worse than fit1 (or in other words, including dist100 is better at predicting the outcome of switch than not including it)."
  },
  {
    "objectID": "contaminated_wells_ex.html#q4-additional-regressions-fit2-fit3-and-fit4",
    "href": "contaminated_wells_ex.html#q4-additional-regressions-fit2-fit3-and-fit4",
    "title": "Comtaminated Wells in Bangladesh",
    "section": "Q4: Additional regressions: fit2, fit3, and fit4",
    "text": "Q4: Additional regressions: fit2, fit3, and fit4\n\na)\n\nfit2 &lt;- glm(switch ~ larsenic, family = binomial(link='logit'), wells)\n\n\n\nb)\n\nfit3 &lt;- glm(switch ~ zeduc, family = binomial(link = 'logit'), wells)\n\n\n\nc)\n\nfit4 &lt;- glm(switch ~ dist100 + larsenic + zeduc, family = binomial(link = 'logit'), wells)\n\n\n\nd)\n\nwell_regs &lt;- list(fit1, fit2, fit3, fit4)\nmodelsummary(well_regs, fmt = 2, \n             title = \"Regression results for wells dataset\")\n\n\nRegression results for wells dataset\n\n\n\n (1)\n  (2)\n  (3)\n  (4)\n\n\n\n\n(Intercept)\n0.61\n0.10\n0.30\n0.53\n\n\n\n(0.06)\n(0.04)\n(0.04)\n(0.06)\n\n\ndist100\n−0.62\n\n\n−0.98\n\n\n\n(0.10)\n\n\n(0.11)\n\n\nlarsenic\n\n0.71\n\n0.89\n\n\n\n\n(0.06)\n\n(0.07)\n\n\nzeduc\n\n\n0.16\n0.17\n\n\n\n\n\n(0.04)\n(0.04)\n\n\nNum.Obs.\n3020\n3020\n3020\n3020\n\n\nAIC\n4080.2\n3993.3\n4104.4\n3886.2\n\n\nBIC\n4092.3\n4005.3\n4116.4\n3910.2\n\n\nLog.Lik.\n−2038.119\n−1994.644\n−2050.193\n−1939.077\n\n\nF\n40.744\n122.106\n17.531\n70.581\n\n\nRMSE\n0.49\n0.48\n0.49\n0.47"
  },
  {
    "objectID": "contaminated_wells_ex.html#q5-interpreting-fit2-fit3-fit4",
    "href": "contaminated_wells_ex.html#q5-interpreting-fit2-fit3-fit4",
    "title": "Comtaminated Wells in Bangladesh",
    "section": "Q5: Interpreting fit2, fit3, fit4",
    "text": "Q5: Interpreting fit2, fit3, fit4\n\na)\n\nwells |&gt;\n  ggplot(aes(x = larsenic, y = switch)) +\n  geom_smooth(method = 'glm', method.args = list(family = \"binomial\")) +\n  geom_jitter(width = 0.5, height = 0.07)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThere is a statistically significant and positive relationship between the log arsenic levels and switch rate. The sign makes intuitive sense because the higher the arsenic levels, the odds of switching should increase.\n\n\nb)\n\nwells |&gt;\n  ggplot(aes(x = zeduc, y = switch)) +\n  geom_smooth(method = \"glm\", method.args = list(family = \"binomial\"))+\n  geom_jitter(width = 0.5, height = 0.1)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nIt seems as if even though there is a statistically significant relationship between the the change of 1 standard deviation of education and the log odds of switching, although the magnitude of this effect is quite small. The sign of its coefficient is positive which may be because: people who have had more education on average is more aware of the long-term negative health effects of contaminated wells. However, it is good to note that this is an average effect and there are many people in the dataset who have 1 standard deviation less than the average education, who do switch to a healthier well.\n\n\nc)\n\nfit4_mean &lt;- wells |&gt;\n  select(dist100, larsenic, zeduc) |&gt;\n  summarize(dist100_mean = mean(dist100), \n            larsenic_mean = mean(larsenic), \n            zeduc_mean = mean(zeduc)) |&gt;\n  mutate(predic_mean = predict(fit4, data.frame(dist100 = dist100_mean, larsenic = larsenic_mean, zeduc = zeduc_mean), type = \"response\"))\n\nmarg_dist &lt;- coef(fit4)[2] * fit4_mean$predic_mean * (1 - fit4_mean$predic_mean)\nmarg_larsenic &lt;- coef(fit4)[3] * fit4_mean$predic_mean * (1 - fit4_mean$predic_mean)\nmarg_zeduc &lt;- coef(fit4)[4] * fit4_mean$predic_mean * (1 - fit4_mean$predic_mean)\nmarg_fit4 &lt;- cbind(marg_dist, marg_larsenic, marg_zeduc)\nrownames(marg_fit4) &lt;- \"marg_effect_fit4\"\n\ndiv_dist &lt;- coef(fit4)[2] / 4\ndiv_larsenic &lt;- coef(fit4)[3] / 4\ndiv_zeduc &lt;- coef(fit4)[4] / 4\ndiv4_fit4 &lt;- cbind(div_dist, div_larsenic, div_zeduc)\nrownames(div4_fit4) &lt;- \"div4_fit4\"\n\ndata.frame(rbind(marg_fit4, div4_fit4))\n\n                 marg_dist marg_larsenic marg_zeduc\nmarg_effect_fit4 -0.238147     0.2162507 0.04212322\ndiv4_fit4        -0.244733     0.2222313 0.04328816\n\n\nIt seems that distance to the well has a slightly larger impact on the probability of switching than the log arsenic levels for someone who has the mean value of all three predictors. The level of education (number of sd from the mean) compared to the other two predictors has minimal impact on the probability of switching, according to this regression. The divide by four rules gives larger marginal effective (by magnitude) than the three marginal effects for the average household."
  },
  {
    "objectID": "contaminated_wells_ex.html#q6-predictive-performance-of-fit4",
    "href": "contaminated_wells_ex.html#q6-predictive-performance-of-fit4",
    "title": "Comtaminated Wells in Bangladesh",
    "section": "Q6: Predictive performance of fit4",
    "text": "Q6: Predictive performance of fit4\n\na)\n\na)\n\nwells &lt;- wells |&gt;\n  mutate(p4 = predict(fit4, type = 'response')) |&gt;\n  select(p4, dist100, everything())\n\n\n\nb)\n\nwells &lt;- wells |&gt; \n  mutate(pred4 = ifelse(p4 &gt; 0.5, 1, 0)) |&gt;\n  select(pred4, everything())\n\nI am going to assume to use the bayes classifer rule for constructing the pred1 ### c)\n\nerror_fit4 &lt;- wells |&gt; \n  summarize(error_rate = mean(pred4 - switch))\nerror_fit4\n\n# A tibble: 1 × 1\n  error_rate\n       &lt;dbl&gt;\n1      0.119\n\n\nThis means that pred1 overestimates the switch rate for approx. 30 percent of the data.\n\n\nd)\n\nconf_mat2 &lt;- table(wells$pred4, wells$switch)\nconf_mat2\n\n   \n       0    1\n  0  546  379\n  1  737 1358\n\n\n\n\ne)\n\nsens_fit4 &lt;- conf_mat2[2,2] / (conf_mat2[2,2] + conf_mat2[1,2])\nspes_fit4 &lt;- conf_mat2[1,1] / (conf_mat2[1,1] + conf_mat2[2,1])\ndata.frame(cbind(sens_fit4, spes_fit4))\n\n  sens_fit4 spes_fit4\n1 0.7818077 0.4255651\n\n\n\n\nf)\n\nwells |&gt;\n  summarize(mean(switch))\n\n# A tibble: 1 × 1\n  `mean(switch)`\n           &lt;dbl&gt;\n1          0.575\n\n\nThis regression has a significant reduction or error compared to the null model, as compared to the null model of everyone switching it has much better spesificity.\n\n\n\nb)\n\nfit4_performance &lt;- data.frame(cbind(sens = sens_fit4, spes = spes_fit4, error_rate = error_fit4))\nrownames(fit4_performance) &lt;- \"fit4_perf\"\n\nfit1_performance &lt;- data.frame(cbind(sens = sens_fit1, spes = spes_fit1, error_rate = error_fit1))\nrownames(fit1_performance) &lt;- \"fit1_perf\"\nrbind(fit1_performance, fit4_performance)\n\n               sens      spes error_rate\nfit1_perf 0.9234312 0.1512081  0.3165563\nfit4_perf 0.7818077 0.4255651  0.1185430\n\n\nAs we can see form this tibble, fit1 has very high sensitivity with very low specificity, while fit4 has lower sensitivity but much better specificity and a lower error rate. This suggests that fit4 is better than fit1 at correctly predicting cases where a household does not switch to a clean well. This improvement also outweighs its lack of predictive power when it comes to correctly predicting a household switching when it actually does switch, as it has a overall lower error rate (which both sensitivity and specificity contributes towards)."
  },
  {
    "objectID": "optimal_stop_ex.html",
    "href": "optimal_stop_ex.html",
    "title": "Optimal Stopping",
    "section": "",
    "text": "The basic thought process behind this version of the simulation setup is that:\n\nRandomly sample the ranks of the phase 1 supervisors\nOrder the list of supervisors from best score to worst score, and select the one from the first value of that vector, as well as his rank (test_list_best)\nThen sample the supervisors in phase 2 by randomly assiging each supervior with a rank, exclusing the ranks that have been given to the phase 1 supervisors (super_list_rank)\nFinally, cycle through each supervior, checking if their rank is higher than the largest out of all professors in phase 1, returning the rank of the processor I end up with.\n\n\nset.seed(69)\nk &lt;- 40\nn &lt;- 50\nsupervisor_sims &lt;- \\(n, k) {\n  test_list &lt;- c(1:k)\n  test_list_rank &lt;- sample(1:n, k, replace = FALSE)\n  test_list_ordered &lt;- test_list[order(test_list_rank, decreasing = TRUE)]\n  test_list_best &lt;- c(test_list_ordered[1], max(test_list_rank))\n  super_list &lt;- c(k+1, n)\n  super_list_rank &lt;- sample(1:n, n, replace = FALSE)\n  super &lt;- which(!super_list_rank %in% test_list_rank)\n  super_list_rank &lt;- super_list_rank[super]\n  i &lt;- 1\n  while (super_list_rank[i] &gt; min(test_list_rank) & i &lt; n-k) {\n    i &lt;- i+1\n  }\n  super_list_rank[i]\n}\nset.seed(69)\nmean(map_dbl(1:100000, \\(i) supervisor_sims(50, 40)))\n\n[1] 20.95339\n\n\n\n\n\nEven though the result on the first is correct (as it matched with the code below when sampling them many times), I found a simplier way to produce the same output. The basic idea is that we sample all the superviors rank in one go (this is probably the easiest way to do it).\n\nset.seed(69)\nsupervisor_sim &lt;- \\(n, k) {\n  # Randomly generate scores for n supervisors\n  supervisors_rank &lt;- sample(1:n, n, replace = FALSE)\n  \n  # Sample k supervisors for phase 1\n  phase_one_rank &lt;- supervisors_rank[1:k]\n  min_rank &lt;- min(phase_one_rank)\n  \n  # Start phase 2\n  for (i in (k+1):n) {\n    if (supervisors_rank[i] &lt; min_rank) {\n      return(supervisors_rank[i])\n    }\n  }\n  \n  # If no supervisor found in phase 2, return last one\n  return(supervisors_rank[n])\n}\n\nmean(map_dbl(1:100000, \\(i) supervisor_sim(50, 40)))\n\n[1] 21.02788"
  },
  {
    "objectID": "optimal_stop_ex.html#q1",
    "href": "optimal_stop_ex.html#q1",
    "title": "Optimal Stopping",
    "section": "",
    "text": "The basic thought process behind this version of the simulation setup is that:\n\nRandomly sample the ranks of the phase 1 supervisors\nOrder the list of supervisors from best score to worst score, and select the one from the first value of that vector, as well as his rank (test_list_best)\nThen sample the supervisors in phase 2 by randomly assiging each supervior with a rank, exclusing the ranks that have been given to the phase 1 supervisors (super_list_rank)\nFinally, cycle through each supervior, checking if their rank is higher than the largest out of all professors in phase 1, returning the rank of the processor I end up with.\n\n\nset.seed(69)\nk &lt;- 40\nn &lt;- 50\nsupervisor_sims &lt;- \\(n, k) {\n  test_list &lt;- c(1:k)\n  test_list_rank &lt;- sample(1:n, k, replace = FALSE)\n  test_list_ordered &lt;- test_list[order(test_list_rank, decreasing = TRUE)]\n  test_list_best &lt;- c(test_list_ordered[1], max(test_list_rank))\n  super_list &lt;- c(k+1, n)\n  super_list_rank &lt;- sample(1:n, n, replace = FALSE)\n  super &lt;- which(!super_list_rank %in% test_list_rank)\n  super_list_rank &lt;- super_list_rank[super]\n  i &lt;- 1\n  while (super_list_rank[i] &gt; min(test_list_rank) & i &lt; n-k) {\n    i &lt;- i+1\n  }\n  super_list_rank[i]\n}\nset.seed(69)\nmean(map_dbl(1:100000, \\(i) supervisor_sims(50, 40)))\n\n[1] 20.95339\n\n\n\n\n\nEven though the result on the first is correct (as it matched with the code below when sampling them many times), I found a simplier way to produce the same output. The basic idea is that we sample all the superviors rank in one go (this is probably the easiest way to do it).\n\nset.seed(69)\nsupervisor_sim &lt;- \\(n, k) {\n  # Randomly generate scores for n supervisors\n  supervisors_rank &lt;- sample(1:n, n, replace = FALSE)\n  \n  # Sample k supervisors for phase 1\n  phase_one_rank &lt;- supervisors_rank[1:k]\n  min_rank &lt;- min(phase_one_rank)\n  \n  # Start phase 2\n  for (i in (k+1):n) {\n    if (supervisors_rank[i] &lt; min_rank) {\n      return(supervisors_rank[i])\n    }\n  }\n  \n  # If no supervisor found in phase 2, return last one\n  return(supervisors_rank[n])\n}\n\nmean(map_dbl(1:100000, \\(i) supervisor_sim(50, 40)))\n\n[1] 21.02788"
  },
  {
    "objectID": "optimal_stop_ex.html#q2",
    "href": "optimal_stop_ex.html#q2",
    "title": "Optimal Stopping",
    "section": "Q2",
    "text": "Q2\nTo do this we just use the map function to sample lots of times and then calculate what percentage of those is ranked at number one\n\nset.seed(6969)\nget_prob_preferred &lt;- \\(n, k, sample_size = 10000) {\n  samples &lt;- map_dbl(1:sample_size, \\(i) supervisor_sim(n, k))\n  sum(samples == 1) / sample_size\n}\nget_prob_preferred(50, 5)\n\n[1] 0.2422\n\n\nJust to check that the first simulation works as well:\n\nset.seed(6969)\nget_prob_preferredd &lt;- \\(n, k, sample_size = 10000) {\n  samples &lt;- map_dbl(1:sample_size, \\(i) supervisor_sims(n, k))\n  sum(samples == 1) / sample_size\n}\nget_prob_preferredd(50, 5)\n\n[1] 0.2399"
  },
  {
    "objectID": "optimal_stop_ex.html#q3",
    "href": "optimal_stop_ex.html#q3",
    "title": "Optimal Stopping",
    "section": "Q3",
    "text": "Q3\n\nset.seed(6969)\nk_50 &lt;- c(5:25)\n\none_map &lt;- \\(k_50, sample_size){\n   map_dbl(k_50, \\(k_50) get_prob_preferred(50, k_50, sample_size))\n}\nn &lt;- 5\nsample_seq &lt;- seq(from = 1e4, by = 5e3, length.out = n)\nnames(sample_seq) &lt;- paste0(\"sample_size:\", seq(from = 1e5, by = 5e4, length.out = n))\n\ndf &lt;- map_dfc(sample_seq, \\(sample_seq) one_map(k_50, sample_seq)) |&gt; \n  mutate(k = c(5:25)) |&gt;\n  pivot_longer(-k, names_to = \"sample\", values_to = \"value\")\n\ndf |&gt;\n  ggplot(aes(x = k, y = value, color = sample)) +\n  geom_line()\n\n\n\n\nOkay… Let’s now look at the 15-23 range…\n\nset.seed(6969)\nk_50 &lt;- c(15:23)\n\none_map &lt;- \\(k_50, sample_size){\n   map_dbl(k_50, \\(k_50) get_prob_preferred(50, k_50, sample_size))\n}\nn &lt;- 5\nsample_seq &lt;- seq(from = 1e5, by = 5e4, length.out = n)\nnames(sample_seq) &lt;- paste0(\"sample_size:\", seq(from = 1e5, by = 5e4, length.out = n))\n\nmap_dfc(sample_seq, \\(sample_seq) one_map(k_50, sample_seq)) |&gt; \n  mutate(k = c(15:23)) |&gt;\n  pivot_longer(-k, names_to = \"sample\", values_to = \"value\") |&gt;\n  ggplot(aes(x = k, y = value, color = sample)) +\n  geom_line()\n\n\n\n\nNow let’s narrow this down further, say 16-21\n\nset.seed(6969)\nk_50 &lt;- c(17:20)\n\none_map &lt;- \\(k_50, sample_size){\n   map_dbl(k_50, \\(k_50) get_prob_preferred(50, k_50, sample_size))\n}\nn &lt;- 5\nsample_seq &lt;- seq(from = 2e5, by = 1e5, length.out = n)\nnames(sample_seq) &lt;- paste0(\"sample_size:\", seq(from = 2e5, by = 1e5, length.out = n))\n\nmap_dfc(sample_seq, \\(sample_seq) one_map(k_50, sample_seq)) |&gt; \n  mutate(k = c(17:20)) |&gt;\n  pivot_longer(-k, names_to = \"sample\", values_to = \"value\") |&gt;\n  ggplot(aes(x = k, y = value, color = sample)) +\n  geom_line()\n\n\n\n\nIt’s hard to say for sure, but it seems as if the optimal value (the number of supervisors when n=50 at phase one) is either at 18 or 19. I’m not sure if there’s a value k that the simulation would converge on and in any case it took really long to run."
  }
]