[
  {
    "objectID": "filedraw_ex.html",
    "href": "filedraw_ex.html",
    "title": "Filedraw Bias",
    "section": "",
    "text": "filedrawer &lt;- read_csv(\"https://ditraglia.com/data/filedrawer.csv\")\n\nRows: 221 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): journal, DV, IV\ndbl (2): id, max.h\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\ndatasummary_crosstab(IV ~ DV, data = filedrawer)\n\n\n\n\n IV\n\nPublished, non top\nPublished, top\nUnpublished\nUnwritten\nAll\n\n\n\n\nNull\nN\n5\n5\n7\n31\n48\n\n\n\n% row\n10.4\n10.4\n14.6\n64.6\n100.0\n\n\nStrong\nN\n35\n21\n31\n4\n91\n\n\n\n% row\n38.5\n23.1\n34.1\n4.4\n100.0\n\n\nWeak\nN\n31\n9\n32\n10\n82\n\n\n\n% row\n37.8\n11.0\n39.0\n12.2\n100.0\n\n\nAll\nN\n71\n35\n70\n45\n221\n\n\n\n% row\n32.1\n15.8\n31.7\n20.4\n100.0\n\n\n\n\n\n\n\n\n\n\n\nsuccess &lt;- filedrawer |&gt;\n  select(IV, DV) |&gt;\n  group_by(IV) |&gt;\n  summarize(unpublished = sum(as.numeric(DV == \"Unpublished\")), \n            published_or_unwritten = sum(as.numeric(DV != \"Unpublished\"))) |&gt;\n  summarize(IV = IV, publication_rate = published_or_unwritten / (published_or_unwritten + unpublished)) \n\nWarning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\ndplyr 1.1.0.\nℹ Please use `reframe()` instead.\nℹ When switching from `summarise()` to `reframe()`, remember that `reframe()`\n  always returns an ungrouped data frame and adjust accordingly.\n\nsample_sizes &lt;- c(sum(as.numeric(filedrawer$IV == \"Null\")),\nsum(as.numeric(filedrawer$IV == \"Weak\")),\nsum(as.numeric(filedrawer$IV == \"Strong\")))\n\nresult &lt;- prop.test(success$publication_rate, sample_sizes)\n\nWarning in prop.test(success$publication_rate, sample_sizes): Chi-squared\napproximation may be incorrect\n\ntidy(result) |&gt;\n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nestimate1\nestimate2\nestimate3\nstatistic\np.value\nparameter\nmethod\nalternative\n\n\n\n\n0.0177951\n0.0080407\n0.0067006\n0.4401403\n0.8024625\n2\n3-sample test for equality of proportions without continuity correction\ntwo.sided\n\n\n\n\n\nAs we can see, the p value of these three values equally each other is .8, which could easily be rejected at any conventional significance level.\n\nfiledrawer &lt;- filedrawer |&gt;\n  mutate(DV_bin = ifelse(filedrawer$DV %in% c(\"Published, non top\", \"Published, top\"), 1, 0), \n         null = ifelse(filedrawer$IV == \"Null\", 1, 0), \n         weak = ifelse(filedrawer$IV == \"Weak\", 1, 0)) \n\nlreg_pub &lt;- glm(DV_bin ~ null + weak, family = binomial(link = 'logit'), filedrawer)\ntidy(lreg_pub) |&gt;\n  mutate(odds_of_publish = exp(estimate)) |&gt;\n  select(term, odds_of_publish, estimate, statistic) |&gt;\n  knitr::kable()\n\n\n\n\nterm\nodds_of_publish\nestimate\nstatistic\n\n\n\n\n(Intercept)\n1.6000000\n0.4700036\n2.181266\n\n\nnull\n0.1644737\n-1.8050047\n-4.342867\n\n\nweak\n0.5952381\n-0.5187938\n-1.681084\n\n\n\n\n\nThrough this we can see that the odds of publishing for a result that has no findings (null) is 0.16 times of that of the odds of the strong results. Additionally, we see that the odds of publishing for a result that has weak findings (weak) is 0.16 times of that of the odds of strong results.\n\n\n\nWe just have to add this in as a control to our previous regression:\n\nlreg_pub2 &lt;- glm(DV_bin ~ null + weak + max.h, family = binomial(link = 'logit'), filedrawer)\nlreg_pubs &lt;- list(lreg_pub, lreg_pub2)\nmodelsummary(lreg_pubs, fmt = 3, gof_omit = \"Num.Obs.|AIC|BIC|Log.Lik|F\")\n\n\n\n\n\n (1)\n  (2)\n\n\n\n\n(Intercept)\n0.470\n0.022\n\n\n\n(0.215)\n(0.270)\n\n\nnull\n−1.805\n−2.029\n\n\n\n(0.416)\n(0.441)\n\n\nweak\n−0.519\n−0.415\n\n\n\n(0.309)\n(0.315)\n\n\nmax.h\n\n0.022\n\n\n\n\n(0.008)\n\n\nRMSE\n0.48\n0.47\n\n\n\n\n\n\n\n\n\n\nThese two regressions seems to suggest that there is a statistically significant relationship between max.h and the rate of publish. However, controlling for the H index did not weaken the other factors, as weak is still statistically insignificant and the effect of null compared to strong is even stronger."
  },
  {
    "objectID": "filedraw_ex.html#q1-patterns-in-filedrawer.csv",
    "href": "filedraw_ex.html#q1-patterns-in-filedrawer.csv",
    "title": "Filedraw Bias",
    "section": "",
    "text": "filedrawer &lt;- read_csv(\"https://ditraglia.com/data/filedrawer.csv\")\n\nRows: 221 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): journal, DV, IV\ndbl (2): id, max.h\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\ndatasummary_crosstab(IV ~ DV, data = filedrawer)\n\n\n\n\n IV\n\nPublished, non top\nPublished, top\nUnpublished\nUnwritten\nAll\n\n\n\n\nNull\nN\n5\n5\n7\n31\n48\n\n\n\n% row\n10.4\n10.4\n14.6\n64.6\n100.0\n\n\nStrong\nN\n35\n21\n31\n4\n91\n\n\n\n% row\n38.5\n23.1\n34.1\n4.4\n100.0\n\n\nWeak\nN\n31\n9\n32\n10\n82\n\n\n\n% row\n37.8\n11.0\n39.0\n12.2\n100.0\n\n\nAll\nN\n71\n35\n70\n45\n221\n\n\n\n% row\n32.1\n15.8\n31.7\n20.4\n100.0\n\n\n\n\n\n\n\n\n\n\n\nsuccess &lt;- filedrawer |&gt;\n  select(IV, DV) |&gt;\n  group_by(IV) |&gt;\n  summarize(unpublished = sum(as.numeric(DV == \"Unpublished\")), \n            published_or_unwritten = sum(as.numeric(DV != \"Unpublished\"))) |&gt;\n  summarize(IV = IV, publication_rate = published_or_unwritten / (published_or_unwritten + unpublished)) \n\nWarning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\ndplyr 1.1.0.\nℹ Please use `reframe()` instead.\nℹ When switching from `summarise()` to `reframe()`, remember that `reframe()`\n  always returns an ungrouped data frame and adjust accordingly.\n\nsample_sizes &lt;- c(sum(as.numeric(filedrawer$IV == \"Null\")),\nsum(as.numeric(filedrawer$IV == \"Weak\")),\nsum(as.numeric(filedrawer$IV == \"Strong\")))\n\nresult &lt;- prop.test(success$publication_rate, sample_sizes)\n\nWarning in prop.test(success$publication_rate, sample_sizes): Chi-squared\napproximation may be incorrect\n\ntidy(result) |&gt;\n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nestimate1\nestimate2\nestimate3\nstatistic\np.value\nparameter\nmethod\nalternative\n\n\n\n\n0.0177951\n0.0080407\n0.0067006\n0.4401403\n0.8024625\n2\n3-sample test for equality of proportions without continuity correction\ntwo.sided\n\n\n\n\n\nAs we can see, the p value of these three values equally each other is .8, which could easily be rejected at any conventional significance level.\n\nfiledrawer &lt;- filedrawer |&gt;\n  mutate(DV_bin = ifelse(filedrawer$DV %in% c(\"Published, non top\", \"Published, top\"), 1, 0), \n         null = ifelse(filedrawer$IV == \"Null\", 1, 0), \n         weak = ifelse(filedrawer$IV == \"Weak\", 1, 0)) \n\nlreg_pub &lt;- glm(DV_bin ~ null + weak, family = binomial(link = 'logit'), filedrawer)\ntidy(lreg_pub) |&gt;\n  mutate(odds_of_publish = exp(estimate)) |&gt;\n  select(term, odds_of_publish, estimate, statistic) |&gt;\n  knitr::kable()\n\n\n\n\nterm\nodds_of_publish\nestimate\nstatistic\n\n\n\n\n(Intercept)\n1.6000000\n0.4700036\n2.181266\n\n\nnull\n0.1644737\n-1.8050047\n-4.342867\n\n\nweak\n0.5952381\n-0.5187938\n-1.681084\n\n\n\n\n\nThrough this we can see that the odds of publishing for a result that has no findings (null) is 0.16 times of that of the odds of the strong results. Additionally, we see that the odds of publishing for a result that has weak findings (weak) is 0.16 times of that of the odds of strong results.\n\n\n\nWe just have to add this in as a control to our previous regression:\n\nlreg_pub2 &lt;- glm(DV_bin ~ null + weak + max.h, family = binomial(link = 'logit'), filedrawer)\nlreg_pubs &lt;- list(lreg_pub, lreg_pub2)\nmodelsummary(lreg_pubs, fmt = 3, gof_omit = \"Num.Obs.|AIC|BIC|Log.Lik|F\")\n\n\n\n\n\n (1)\n  (2)\n\n\n\n\n(Intercept)\n0.470\n0.022\n\n\n\n(0.215)\n(0.270)\n\n\nnull\n−1.805\n−2.029\n\n\n\n(0.416)\n(0.441)\n\n\nweak\n−0.519\n−0.415\n\n\n\n(0.309)\n(0.315)\n\n\nmax.h\n\n0.022\n\n\n\n\n(0.008)\n\n\nRMSE\n0.48\n0.47\n\n\n\n\n\n\n\n\n\n\nThese two regressions seems to suggest that there is a statistically significant relationship between max.h and the rate of publish. However, controlling for the H index did not weaken the other factors, as weak is still statistically insignificant and the effect of null compared to strong is even stronger."
  },
  {
    "objectID": "filedraw_ex.html#q2-patterns-in-published.csv",
    "href": "filedraw_ex.html#q2-patterns-in-published.csv",
    "title": "Filedraw Bias",
    "section": "Q2: Patterns in published.csv",
    "text": "Q2: Patterns in published.csv\n\na.\n\npublished &lt;- read_csv(\"https://ditraglia.com/data/published.csv\")\n\nRows: 53 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (5): id.p, cond.s, out.s, cond.p, out.p\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nb.\n\npublished |&gt;\n  ggplot(aes(x = cond.s, y = cond.p))+\n  geom_jitter(width = 0.1, height = 0.1) +\n  geom_abline(a=0, b=1, col=\"red\")\n\nWarning in geom_abline(a = 0, b = 1, col = \"red\"): Ignoring unknown parameters:\n`a` and `b`\n\n\n\n\n\n\n\nc.\n\npublished |&gt;\n  ggplot(aes(x = out.s, y = out.p))+\n  geom_jitter(width = 0.1, height = 0.1)+\n  geom_abline(a=0, b=1, col=\"red\")\n\nWarning in geom_abline(a = 0, b = 1, col = \"red\"): Ignoring unknown parameters:\n`a` and `b`\n\n\n\n\n\n\n\nd.\nBoth plots suggests that there is a systematic, downward bias wuch that both have all of the points either on or below the 45 degree line. The points on the 45 degree line imply truthfulness, that all experimental conditions in the study are published and all variables inthe study are published. It is impossible to publish more points than those included in the study so natrually there are no points above this line (in red). Furthermore, we see many points especially in the second plot to be below the red line, indicating that there are conditions and variables which was studied but not published. This may be because journal editors and referees are more willing to accept a paper if it has significant results, so either researchers or editors would exclude the conditions and variables which are weak to have a better chance of publication."
  },
  {
    "objectID": "filedraw_ex.html#q3",
    "href": "filedraw_ex.html#q3",
    "title": "Filedraw Bias",
    "section": "Q3",
    "text": "Q3\n\na. b. c.\nIf we want to find the probability of rejecting at least one null, we first have to find the probability of rejecting no null (accepting every null), and use one to subtract that value. Similarly, if we want to find the probability of rejecting at least three nulls, we have to find the probability of rejecting at most two nulls, which means the probability of accepting n-2 nulls.\n\noutcomes &lt;- length(published$out.s)\nconditions &lt;- length(published$cond.s)\nalpha &lt;- 0.05\ntotal_hypothesis &lt;- outcomes * conditions\n\np_one &lt;- 1 - (0.95^total_hypothesis)\np_two &lt;- 1 - (0.95^(total_hypothesis - 1))\np_three &lt;- 1 - (0.95^(total_hypothesis - 2))\ndata.frame(p_one, p_two, p_three)\n\n  p_one p_two p_three\n1     1     1       1\n\n\nOn average we should see all three (at least reject one, two, and three hypothesis) should have a 100% probability. To check let’s run a simulation to see how this probability changes when the number of least rejections increase.\n\nhypo_test_func &lt;- \\(total_hypothesis, n){\np_n &lt;- 1 - (0.95^(total_hypothesis - n + 1))\np_n\n}\n\nresult &lt;- map_dbl(2400:2809, \\(n) hypo_test_func(total_hypothesis, n))\ndata.frame(reject = c(2400:2809), result = result) |&gt;\n  ggplot(aes(x = reject, y = result)) +\n  geom_line()\n\n\n\n\n\n\nd.\nThe diagram above shows that we should on average reject more hypothesis than we do not, however the opposite occurs in journals that incentivise behavior to only show significant results. The findings highlight the potential for publication bias and p-hacking, where researchers might selectively report only the significant results from a large number of tests, leading to an overrepresentation of false positives in the published literature. This is why it is important to pre-register studies and hypothesis tests and to report all results, regardless of whether they are statistically significant."
  },
  {
    "objectID": "Lumberjack.html",
    "href": "Lumberjack.html",
    "title": "Lumberjack’s Simulation - Quant Trading Interview",
    "section": "",
    "text": "I recently took up an interest in potentially working as a quant. So naturally I began to watch other people doing quant interviews on YouTube. The thinking is that if I can beat them to solving the problem, then I (a lousy PPEist) probably have a chance at a career as a quant trader or something, right? So one day after work I sat down and started watching a Jane Street “mock interview” conducted by two employees, one asks the other:\n“A lumberjack cuts a piece of wood of 1 meter in length at 2 uniformly randomly selected locations along the length of the wood. Find the probably the shortest piece of wood is at most 5 centimeters.”\nThe interviewee was able to derive the solution within minutes. I watched in horror at the speed in which (who I assume is) the quant-trader was able to derive the solution. I was finished, I thought.\nBut wait… I’ve recently learned how to run simulations through an R course I audited with my economics professor. Maybe I could simulate this question to get the right answer.\nAfter hesitating on whether this is really the best way to spend my evening, I opened RStudio."
  },
  {
    "objectID": "Lumberjack.html#simulation-1",
    "href": "Lumberjack.html#simulation-1",
    "title": "Lumberjack’s Simulation - Quant Trading Interview",
    "section": "Simulation #1",
    "text": "Simulation #1\nOkay so what if we take two random draws from a standard normal uniform distribution (a distribution that spits out random numbers from 0 to 1 with equal probability) and then check if their sum is larger or smaller than 1. If they are larger than 1 we discard them, and if they are smaller than 1 then that makes a valid sample!\n\nset.seed(69)\nnum_simulations &lt;- 1e5\nlumber_sim &lt;- \\(num_simulations = 1e5) {\n  count &lt;- 0\n  result_vec &lt;- numeric(num_simulations)\n  while(count &lt; num_simulations){\n    locations &lt;- runif(2)\n    if(sum(locations) &lt; 1) {\n    result &lt;- tail(sort(c(locations, 1 - sum(locations)), decreasing = TRUE), 1)\n    count &lt;- count + 1\n    result_vec[count] &lt;- result\n      }\n  }\n  return(sum(result_vec &lt;= 0.05) / num_simulations)\n}\n\nlumber_sim()\n\n[1] 0.27425\n\n\nAnd there we have it! If only I can bring a computer with me to a quant interview… Anyway, at this point I’ve already spent an embarrassing amount of time on this “mock interview question”, but my curiosity pushed me to keep procrastinating. How would this percentile change if I had to cut the 1 meter log into more and more pieces? This seems like a harder question that seems impossible for the entry level quant analyst to derive. Solving this question would surely boost my ego. So I started thinking…"
  },
  {
    "objectID": "Lumberjack.html#simulation-2-back-to-the-drawing-board",
    "href": "Lumberjack.html#simulation-2-back-to-the-drawing-board",
    "title": "Lumberjack’s Simulation - Quant Trading Interview",
    "section": "Simulation #2: Back to the Drawing Board",
    "text": "Simulation #2: Back to the Drawing Board\nAt this point I’m having second doubts about my original strategy. As the number of cuts on the 1 meter log increases, the probability that the sum of a bunch of uniform distributions surpassing one will increase as well. So say that I break the plank down to 100 bits: there’s no way it is a good idea to simulate this by taking random draws from the uniform distribution 99 times and testing if their sum is greater than one (that would also probably take too long). Is there another way?\nSuddenly a thought came to me: what if, to select the three values, we somehow drew twice and used their difference. Okay this could work I’m pretty sure, we just have to construct this difference term in a way such that in combination with the two other terms it adds to 1, per the questions request. So suppose we drew two values $a,b (0,1) $ and then took their difference \\(b-a\\). The other two values would obviously be \\(a\\) and \\(1-b\\) since these two values are both smaller than 1 and the three sum to one. One thing to be careful though, we somehow have to order \\(a\\) and \\(b\\) to make sure \\(b&gt;a\\) so that \\(b-a &gt;0\\). If we get this simplified model down we can then (and if the simulated proportion match with the value from the simulation above), scaling this model should be easier.\n\ncut_log_randomly &lt;- function() {\n  # Generate two random points between 0 and 1\n  cuts &lt;- sort(runif(2))\n  \n  # Calculate the lengths of the three pieces\n  piece1 &lt;- cuts[1]\n  piece2 &lt;- cuts[2] - cuts[1]\n  piece3 &lt;- 1 - cuts[2]\n  \n  # Return the lengths of the three pieces\n  a &lt;- tail(sort(c(piece1, piece2, piece3), decreasing = TRUE), 1)\n  return(a)\n}\n\nresult &lt;- cut_log_randomly()\nres_vec &lt;- map(1:num_simulations, \\(i) cut_log_randomly())\nsum(res_vec &lt;= 0.05) / num_simulations\n\n[1] 0.27663\n\n\nCool, the two values match up which means we’re in the good! Now time to generalize (oof…)"
  },
  {
    "objectID": "Lumberjack.html#simulation-3-scaling",
    "href": "Lumberjack.html#simulation-3-scaling",
    "title": "Lumberjack’s Simulation - Quant Trading Interview",
    "section": "Simulation 3: Scaling",
    "text": "Simulation 3: Scaling\nFirst lets try to write a function, aiming to achieve two things:\n\nGiven an input of the number of wood chunks it spits out some randomly generated length that add up to 1\nAlso out of these chunks if it also gave us just the smallest one, that would be useful the future\n\nThe idea is that to scale the previous simulation we first generate a bunch of uniform(0,1) draws and then order them from smallest to largest. Then we can subtract the i+1 by the ith term and 1 by the last term. Let’s implement this strategy.\n\ncut_log_randomly_n_pieces &lt;- function(n) {\n  if (n &lt;= 1) {\n    stop(\"Number of pieces should be greater than 1.\")\n  }\n  \n  # Generate n-1 random points between 0 and 1\n  cuts &lt;- sort(runif(n - 1))\n  \n  # Calculate the lengths of the n pieces\n  differences &lt;- diff(cuts)\n  if (n &gt; 2) {\n  names_diff &lt;- setNames(differences, c(paste0(\"len_\", 2:(length(cuts))))) }\n  else {names_diff &lt;- differences}\n  final_value &lt;- 1 - cuts[length(cuts)]\n  name_n &lt;- setNames(final_value, c(paste0(\"len_\", n)))\n  lengths &lt;- c(\"len_1\" = cuts[1], names_diff, name_n)\n  smallest &lt;- tail(sort(lengths, decreasing = TRUE), 1)\n  # Return the lengths of smallest\n  return(list(smallest = smallest, lengths = lengths))\n}\n\ncut_log_randomly_n_pieces(3)\n\n$smallest\n    len_3 \n0.0079887 \n\n$lengths\n    len_1     len_2     len_3 \n0.8581453 0.1338660 0.0079887 \n\n\nSo after a few attempts I got this to work. What was especially difficult was that when we want two parts of the wood the middle term c(paste0(\"len\", 2:length(cuts)))) in the name_diff line would try to give a third name, so the length of the name vector and actual vector did not line up. This was a royal pain in the posterior to find and take care of. Other than that it was smooth sailing.\nAt this point though I was worried: intuitively it seems that this way of making random wooden chunks off the 1 meter wooden plank isn’t random enough. What if some chunks are systematically larger than others? Is this really a good way to break the wood up into chunks? So I double checked.\n\nsims &lt;- 1e4\nsim_draws &lt;- map_dfr(1:sims, \\(i) cut_log_randomly_n_pieces(5)[[2]])\nsim_draws|&gt;\n  summarize(across(everything(), mean, .names = '{.col}_mean'))\n\n# A tibble: 1 × 5\n  len_1_mean len_2_mean len_3_mean len_4_mean len_5_mean\n       &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1      0.202      0.198      0.202      0.198      0.200\n\n\nPhew… Turns out that (at least if we want five chunks) if we repeated this simulation a bunch of times then each part appraches 1/5. This is a good sign as it shows that we are not systematcally making one chunk smaller or larger than the rest, which would destroy the internal validity of our results later. Cool: marching onward. First let’s try the probably the shortest piece of wood is at most 5 centimeters when the wooden plank is divided into 5 instead of three parts. Let’s also make this a function.\n\nmappings &lt;- map(1:sims, \\(i) cut_log_randomly_n_pieces(5)[[1]])\npercentage &lt;- sum(mappings &lt;= 0.05) / sims\npercentage\n\n[1] 0.692\n\n\nFantastic! now we can say with confidence that at this point, if the interviewer of Jane Street gave me a slightly modified version fo this question, I could secretly consult my program to beat the quant trader. Victory and dominance is mine at last. I have became what, as Jordan Peterson put it, a “high serotonin lobster”."
  },
  {
    "objectID": "Lumberjack.html#simulation-5",
    "href": "Lumberjack.html#simulation-5",
    "title": "Lumberjack’s Simulation - Quant Trading Interview",
    "section": "Simulation #5:",
    "text": "Simulation #5:\nNow let’s combine everything from above and write a function that spits out the solution directly given the nuber of simulations we want to run and the how many chuncks we want to plank to be split into:\n\npercentage_func &lt;- \\(sims_func = 1000, n_func){\n  map &lt;- map(1:sims_func, \\(i) cut_log_randomly_n_pieces(n_func)[[1]])\n  perc &lt;- sum(map &lt;= 0.05) / sims_func\n  setNames(perc, \"percentage_below_0.05\")\n}\n\npercentage_func(10000,3)\n\npercentage_below_0.05 \n               0.2742"
  },
  {
    "objectID": "Lumberjack.html#simulations-within-simulations-just-like-inception",
    "href": "Lumberjack.html#simulations-within-simulations-just-like-inception",
    "title": "Lumberjack’s Simulation - Quant Trading Interview",
    "section": "Simulations within simulations (just like inception)",
    "text": "Simulations within simulations (just like inception)\nFinally, it’ll be cool if we can make a chart where if we specify the range of planks, then we get a table with the solutions of the question. This requires a map within a map (or a while loop within a while loop). Let’s give it some parameters n_vary and sims_func_final and create such a chat using dplyr\n\nn_vary &lt;- 15\nsims_func_final &lt;- 1000\n\nfinal &lt;- 2:n_vary\ndata_points &lt;- final |&gt;\n  map_dfr(\\(x) percentage_func(sims_func = sims_func_final, x)) |&gt;\n  mutate(num_n = 2:n_vary)\n\ndata_points\n\n# A tibble: 14 × 2\n   percentage_below_0.05 num_n\n                   &lt;dbl&gt; &lt;int&gt;\n 1                 0.101     2\n 2                 0.298     3\n 3                 0.488     4\n 4                 0.697     5\n 5                 0.828     6\n 6                 0.941     7\n 7                 0.965     8\n 8                 0.987     9\n 9                 0.998    10\n10                 1        11\n11                 1        12\n12                 1        13\n13                 1        14\n14                 1        15"
  },
  {
    "objectID": "Lumberjack.html#inference",
    "href": "Lumberjack.html#inference",
    "title": "Lumberjack’s Simulation - Quant Trading Interview",
    "section": "Inference",
    "text": "Inference\n\nreg1 &lt;- lm(percentage_below_0.05 ~ log(log(num_n)), data_points)\nreg2 &lt;- lm(percentage_below_0.05 ~ log(num_n), data_points)\nreg3 &lt;- lm(percentage_below_0.05 ~ num_n, data_points)\ncomb_regressions &lt;- list(reg1, reg2, reg3)\nmodelsummary(comb_regressions, fmt = 2)\n\n\n\n\n\n (1)\n  (2)\n  (3)\n\n\n\n\n(Intercept)\n0.33\n−0.12\n0.30\n\n\n\n(0.04)\n(0.10)\n(0.11)\n\n\nlog(log(num_n))\n0.75\n\n\n\n\n\n(0.05)\n\n\n\n\nlog(num_n)\n\n0.46\n\n\n\n\n\n(0.05)\n\n\n\nnum_n\n\n\n0.06\n\n\n\n\n\n(0.01)\n\n\nNum.Obs.\n14\n14\n14\n\n\nR2\n0.948\n0.895\n0.695\n\n\nR2 Adj.\n0.944\n0.886\n0.669\n\n\nAIC\n−30.4\n−20.5\n−5.6\n\n\nBIC\n−28.5\n−18.6\n−3.7\n\n\nLog.Lik.\n18.215\n13.255\n5.811\n\n\nF\n219.093\n101.775\n27.280\n\n\nRMSE\n0.07\n0.09\n0.16\n\n\n\n\n\n\n\nSo MSE goes down as we add more logs, but keep in mind that we are using the same data to test for and build this model, so we’re bound to get some overfitting… This is where machine learning comes in as it takes a bunch of subsets to train and test, then averages their RMSE! Let’s use k fold cross validation to fit the data.\n\nMachine Learning: k-fold Cross Validation\nLet’s try to use this technique in machine learning, which does the following to solve the issue of overfitting:\n\nData Splitting: The original dataset is randomly divided into five subsets of roughly equal size. Each subset is called a “fold.”\nTraining and Testing: The modeling process is performed five times, each time using four folds (80% of the data) for training the model and the remaining one fold (20% of the data) for testing the model’s performance.\nModel Training: In each iteration, the model is trained on four folds, and the training data is used to fit the model and learn the underlying patterns in the data.\nModel Evaluation: The model’s performance is evaluated on the fifth fold (the test set) by calculating a performance metric, such as accuracy, precision, recall, or mean squared error, depending on the type of problem (classification or regression).\nPerformance Aggregation: After all five iterations are completed, the performance metrics from each fold are averaged to obtain a single performance estimate for the model.\n\n\n# Load required libraries\nlibrary(caret)\n\nmin_degree &lt;- 1\nmax_degree &lt;- 11\ndegrees &lt;- min_degree:max_degree\n\n# Set up k-fold cross-validation\nk_fold_control &lt;- trainControl(method = 'cv', number = 5)\n\nmodel_results &lt;- vector(\"list\", length = length(degrees))\n\n# Train the models using k-fold cross-validation for different polynomial degrees\nfor (i in seq_along(degrees)) {\n  # Define the polynomial formula for the current degree\n  formula &lt;- as.formula(paste(\"percentage_below_0.05 ~ poly(num_n,\", degrees[i], \")\"))\n  \n  # Train the model using the current formula\n  model_results[[i]] &lt;- train(formula, data = data_points, method = \"lm\", trControl = k_fold_control)\n}\n\nWarning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,\n: There were missing values in resampled performance measures.\n\nWarning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,\n: There were missing values in resampled performance measures.\n\nWarning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,\n: There were missing values in resampled performance measures.\n\nWarning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,\n: There were missing values in resampled performance measures.\n\n\nWarning in predict.lm(modelFit, newdata): prediction from a rank-deficient fit\nmay be misleading\n\nWarning in predict.lm(modelFit, newdata): prediction from a rank-deficient fit\nmay be misleading\n\nWarning in predict.lm(modelFit, newdata): prediction from a rank-deficient fit\nmay be misleading\n\nWarning in predict.lm(modelFit, newdata): prediction from a rank-deficient fit\nmay be misleading\n\n# Print the cross-validated performance metrics\nrmse_values &lt;- map_dbl(degrees, \\(degrees) model_results[[degrees]]$results$RMSE)\nrmse_df &lt;- data.frame(degree = degrees, RMSE = rmse_values)\nrmse_df |&gt;\n  ggplot(aes(x = degree, y = RMSE)) +\n  geom_point() +\n  geom_line()\n\n\n\n\nAs you can see, as the degree of polynomial increases the RMSE decreases (reduction in bias), but after the 6th polynomial clearly the model starts overfitting the data such that the test set has high error. As there is not a massive difference between 2-7 degrees of polynomial, let’s take the third polynomial in this case for simplicity visualize such a model\n\ndata_points |&gt;\n  ggplot(aes(y = percentage_below_0.05, x = final)) +\n  geom_point() +\n  geom_smooth(method = 'lm', formula = y ~ poly(x, 3), se = FALSE) +\n  labs(title = 'Simulation Results - Cutting Wood',\n       caption = '**simulated n cuts (n+1 wooden parts that add up to 1)') + \n  xlab('Number of peices of wood') +\n  ylab('Probability that the shortest peice is below 0.05')\n\n\n\n\n\n\nRunning 5 regressions to check\nFinally, let’s compare the 5 regressions of using polynomials to fit the data from the 1st to the 5th degree. At this point I’m sorta tired and ready for bed, and the results don’t match up with the CV results. Personally I trust CV better than something like adjusted R^2 which relies on rules of thumb.\n\ncv_regression_5 &lt;- lm(percentage_below_0.05 ~ final + I(final^2) + I(final^3) + I(final^4) + I(final^5), data_points)\ncv_regression_4 &lt;- lm(percentage_below_0.05 ~ final + I(final^2) + I(final^3) + I(final^4), data_points)\ncv_regression_3 &lt;- lm(percentage_below_0.05 ~ final + I(final^2) + I(final^3), data_points)\ncv_regression_2 &lt;- lm(percentage_below_0.05 ~ final + I(final^2), data_points)\ncv_regression_1 &lt;- lm(percentage_below_0.05 ~ final, data_points)\ncomb_regressions_cv &lt;- list(cv_regression_1, cv_regression_2, cv_regression_3, cv_regression_4, cv_regression_5)\n\nmodelsummary(comb_regressions_cv, gof_omit = 'Log.Lik|AIC|BIC|F', fmt = 2, title = 'Regression results for the degree of polynomial', notes = \"I'm personally getting confused here so let's call it a day\")\n\n\nRegression results for the degree of polynomial\n\n\n\n (1)\n  (2)\n  (3)\n  (4)\n  (5)\n\n\n\n\n(Intercept)\n0.30\n−0.29\n−0.62\n−0.47\n0.00\n\n\n\n(0.11)\n(0.07)\n(0.06)\n(0.10)\n(0.10)\n\n\nfinal\n0.06\n0.24\n0.41\n0.30\n−0.13\n\n\n\n(0.01)\n(0.02)\n(0.03)\n(0.07)\n(0.09)\n\n\nI(final^2)\n\n−0.01\n−0.03\n−0.01\n0.13\n\n\n\n\n(0.00)\n(0.00)\n(0.01)\n(0.03)\n\n\nI(final^3)\n\n\n0.00\n0.00\n−0.02\n\n\n\n\n\n(0.00)\n(0.00)\n(0.00)\n\n\nI(final^4)\n\n\n\n0.00\n0.00\n\n\n\n\n\n\n(0.00)\n(0.00)\n\n\nI(final^5)\n\n\n\n\n0.00\n\n\n\n\n\n\n\n(0.00)\n\n\nNum.Obs.\n14\n14\n14\n14\n14\n\n\nR2\n0.695\n0.971\n0.995\n0.996\n0.999\n\n\nR2 Adj.\n0.669\n0.966\n0.993\n0.995\n0.999\n\n\nRMSE\n0.16\n0.05\n0.02\n0.02\n0.01\n\n\n\n I'm personally getting confused here so let's call it a day"
  },
  {
    "objectID": "labor_market_ex.html",
    "href": "labor_market_ex.html",
    "title": "Bias in the Labor Market",
    "section": "",
    "text": "bm &lt;- read_csv('https://ditraglia.com/data/lakisha_aer.csv')\n\nRows: 4870 Columns: 65\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (10): id, ad, firstname, sex, race, city, kind, expminreq, schoolreq, ow...\ndbl (55): education, ofjobs, yearsexp, honors, volunteer, military, empholes...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "labor_market_ex.html#q1",
    "href": "labor_market_ex.html#q1",
    "title": "Bias in the Labor Market",
    "section": "Q1",
    "text": "Q1\nThe research question that the researchers are trying to answer is whether if being black compared to being white alone, irrespective of socioeconomic, education, experience, and other factors, affects hiring decisions. The authors use a RCT that randomly assigns white-sounding and black-sounding names to resumes and send them to newspaper hiring advertisements. The authors find that not only is there a 50-percent gap in callback between a white-sounding and black-sounding name, high-quality resumes compared to low-quality ones have a statistically significant difference in callback rates between white and black applicants. This suggests that the gap between Whites and African Americans widens with resume quality. These observed results do not vary across different sectors."
  },
  {
    "objectID": "labor_market_ex.html#q2",
    "href": "labor_market_ex.html#q2",
    "title": "Bias in the Labor Market",
    "section": "Q2",
    "text": "Q2\n\na)\nDisplay the tibble bm. How many rows and columns does it have?\n\nhead(bm)\n\n# A tibble: 6 × 65\n  id    ad    education ofjobs yearsexp honors volunteer military empholes\n  &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 b     1             4      2        6      0         0        0        1\n2 b     1             3      3        6      0         1        1        0\n3 b     1             4      1        6      0         0        0        0\n4 b     1             3      4        6      0         1        0        1\n5 b     1             3      3       22      0         0        0        0\n6 b     1             4      2        6      1         0        0        0\n# ℹ 56 more variables: occupspecific &lt;dbl&gt;, occupbroad &lt;dbl&gt;,\n#   workinschool &lt;dbl&gt;, email &lt;dbl&gt;, computerskills &lt;dbl&gt;, specialskills &lt;dbl&gt;,\n#   firstname &lt;chr&gt;, sex &lt;chr&gt;, race &lt;chr&gt;, h &lt;dbl&gt;, l &lt;dbl&gt;, call &lt;dbl&gt;,\n#   city &lt;chr&gt;, kind &lt;chr&gt;, adid &lt;dbl&gt;, fracblack &lt;dbl&gt;, fracwhite &lt;dbl&gt;,\n#   lmedhhinc &lt;dbl&gt;, fracdropout &lt;dbl&gt;, fraccolp &lt;dbl&gt;, linc &lt;dbl&gt;, col &lt;dbl&gt;,\n#   expminreq &lt;chr&gt;, schoolreq &lt;chr&gt;, eoe &lt;dbl&gt;, parent_sales &lt;dbl&gt;,\n#   parent_emp &lt;dbl&gt;, branch_sales &lt;dbl&gt;, branch_emp &lt;dbl&gt;, fed &lt;dbl&gt;, …\n\nnrow(bm)\n\n[1] 4870\n\nncol(bm)\n\n[1] 65\n\n\nWe see from these commands that the tibble has 4870 rows and 65 columns\n\n\nb)\nDisplay only the columns sex, race and firstname of bm. What information do these columns contain? How are sex and race encoded?\n\nbm |&gt;\n  select(sex, race, firstname) \n\n# A tibble: 4,870 × 3\n   sex   race  firstname\n   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;    \n 1 f     w     Allison  \n 2 f     w     Kristen  \n 3 f     b     Lakisha  \n 4 f     b     Latonya  \n 5 f     w     Carrie   \n 6 m     w     Jay      \n 7 f     w     Jill     \n 8 f     b     Kenya    \n 9 f     b     Latonya  \n10 m     b     Tyrone   \n# ℹ 4,860 more rows\n\n\nThis tibble contains the sex, race of the individual corresponding to their first name. sex is encoded as m for male and f for female. race is encoded as w for white and b for black.\n\n\nc) Add two new columns to bm: female should take the value TRUE if sex is female, and black should take value TRUE if race is black.\nTo do this we need to create logic vectors and test (iterate) through the sex and race columns to determine if each other them are female or black. Then we use mutate() to add the two columns in the tibble.\n\nsex_f &lt;- bm[[\"sex\"]] == \"f\"\nrace_b &lt;- bm[[\"race\"]] == \"b\"\n\nbm |&gt;\n  select(sex, race, firstname)|&gt;\n  mutate(sex_f, race_b)\n\n# A tibble: 4,870 × 5\n   sex   race  firstname sex_f race_b\n   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;     &lt;lgl&gt; &lt;lgl&gt; \n 1 f     w     Allison   TRUE  FALSE \n 2 f     w     Kristen   TRUE  FALSE \n 3 f     b     Lakisha   TRUE  TRUE  \n 4 f     b     Latonya   TRUE  TRUE  \n 5 f     w     Carrie    TRUE  FALSE \n 6 m     w     Jay       FALSE FALSE \n 7 f     w     Jill      TRUE  FALSE \n 8 f     b     Kenya     TRUE  TRUE  \n 9 f     b     Latonya   TRUE  TRUE  \n10 m     b     Tyrone    FALSE TRUE  \n# ℹ 4,860 more rows"
  },
  {
    "objectID": "labor_market_ex.html#q3",
    "href": "labor_market_ex.html#q3",
    "title": "Bias in the Labor Market",
    "section": "Q3",
    "text": "Q3\n\na)\nHow did the experimenters create their bank of resumes for the experiment?\nThe experimenters started with resumes of actual job searchers but alter them sufficiently to create distinct resumes. The actual job resumes were taken from two job search websites that posts resumes of applicants seeking jobs in Boston and Chicago, with some further occupational and application time constraints.\n\n\nb)\nThe experimenters classified the resumes into two groups. What were they and how did they make the classification?\nThe two groups were: high quality and low quality applicants. The distinction is based on criteria such as labor market experience, career profile, existence of gaps in employment, ans skills listed. Additionally, the researchers also added to the high quality applicants a subset of features such as: pre-employment experience, volunteering experience, extra skills, honors, or military service.\n\n\nc)\nHow did the experimenters generate identities for their fictitious job applicants?\nThe experimenters used name frequency data calculated from birth certificates of all babies born in Massachusetts between 1974 and 1979. They then tabulate the data by race to determine which names are distinctively White and African American, then they use the most instinctive, highest frequency names of both races on the resumes."
  },
  {
    "objectID": "labor_market_ex.html#q4",
    "href": "labor_market_ex.html#q4",
    "title": "Bias in the Labor Market",
    "section": "Q4",
    "text": "Q4\n\na)\nIs sex balanced across race?\nWe can first use the group_by function to sort the tibble by race then sum the number of females in each race bracket via the variable we created in the first part of the sheet sex_f. Summing the logicals treates each TRUE as a 1 and FALSE as 0.\n\nbm |&gt;\n  group_by(race)|&gt;\n  summarize(sum_of_f = sum(sex_f))\n\n# A tibble: 2 × 2\n  race  sum_of_f\n  &lt;chr&gt;    &lt;int&gt;\n1 b         3746\n2 w         3746\n\n\nWe find that the number of males and females are the same across the two racial groups, which means that sex is balanced across race.\n\n\nb)\nAre computer skills balanced across race?\nTo calculate proportion we just take the average across the two groups:\n\nbm |&gt;\n  group_by(race)|&gt;\n  summarize(prop_comp = mean(computerskills))\n\n# A tibble: 2 × 2\n  race  prop_comp\n  &lt;chr&gt;     &lt;dbl&gt;\n1 b         0.832\n2 w         0.809\n\n\nThis shows that the white group has a bit less than the black group, but it is probably not significant as it is a difference of 3% across groups.\n\n\nc)\nAre education and ofjobs balanced across race?\n\nbm |&gt;\n  group_by(race)|&gt;\n  summarize(prop_educ = mean(education), prop_ofjobs = mean(ofjobs))\n\n# A tibble: 2 × 3\n  race  prop_educ prop_ofjobs\n  &lt;chr&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n1 b          3.62        3.66\n2 w          3.62        3.66\n\n\nYes, they are.\n\n\nd)\nCompute the mean and standard deviation of yearsexp by race. Comment on your findings.\n\nbm |&gt;\n  group_by(race)|&gt;\n  summarize(mean_yearsexp = mean(yearsexp), sd_yearsexp = sd(yearsexp))\n\n# A tibble: 2 × 3\n  race  mean_yearsexp sd_yearsexp\n  &lt;chr&gt;         &lt;dbl&gt;       &lt;dbl&gt;\n1 b              7.83        5.01\n2 w              7.86        5.08\n\n\nThis means that not only is mean of years of experience across races the same, but that also the distribution of years of experience around the mean has the same variability, such that we know it is not the case that more individuals the black group have experiences close to the mean of 7.8 years than the white group, and vice versa.\n\n\ne) Why do we care if sex, education, ofjobs, computerskills, and yearsexp are balanced across race?\nWe care because if they are not balanced, then any differences in outcomes (callbacks) may be due to these factors, and not factors directly related to race itself. These variables listed are proxys for if randomization process has eliminated differences in all unobserved factors which may influence the outcome variable. In other words we are testing for the orthogonality condition to hold between the error term and the treatment variable in a casual model.\n\n\nf)\nIs computerskills balanced across sex? What about education? What’s going on here? Is it a problem?\n\nbm |&gt;\n  group_by(sex)|&gt;\n  summarize(mean_computerskills = mean(computerskills),\n            mean_education = mean(education))\n\n# A tibble: 2 × 3\n  sex   mean_computerskills mean_education\n  &lt;chr&gt;               &lt;dbl&gt;          &lt;dbl&gt;\n1 f                   0.868           3.58\n2 m                   0.662           3.73\n\n\nAs we can see, it seems as though females have higher computer skills and low education on average. To investigate why this is: we first see that in section II C of the paper the experimenters use male and female names for sales jobs but nearly exclusively female names for administrative jobs to increase callback rates. To verify this I choose secretary, supervisor, and offsupport as three proxies for adminstrative jobs present in the dataset.\n\nbm |&gt;\n  group_by(sex)|&gt;\n  summarize(sum(secretary), sum(supervisor), sum(offsupport))\n\n# A tibble: 2 × 4\n  sex   `sum(secretary)` `sum(supervisor)` `sum(offsupport)`\n  &lt;chr&gt;            &lt;dbl&gt;             &lt;dbl&gt;             &lt;dbl&gt;\n1 f                 1591               359               556\n2 m                   30                17                22\n\n\nIndeed, we find that for each of these variables there is no men included but exclusively female. Let’s then investigate the computerskills and education for each of these jobs, compared to the rest of the sample (all other jobs).\n\nbm |&gt;\n  group_by(secretary)|&gt;\n  summarize(mean(computerskills), mean(education))\n\n# A tibble: 2 × 3\n  secretary `mean(computerskills)` `mean(education)`\n      &lt;dbl&gt;                  &lt;dbl&gt;             &lt;dbl&gt;\n1         0                  0.752              3.66\n2         1                  0.957              3.54\n\nbm |&gt;\n  group_by(supervisor)|&gt;\n  summarize(mean(computerskills), mean(education))\n\n# A tibble: 2 × 3\n  supervisor `mean(computerskills)` `mean(education)`\n       &lt;dbl&gt;                  &lt;dbl&gt;             &lt;dbl&gt;\n1          0                  0.811              3.63\n2          1                  0.934              3.52\n\nbm |&gt;\n  group_by(offsupport)|&gt;\n  summarize(mean(computerskills), mean(education))\n\n# A tibble: 2 × 3\n  offsupport `mean(computerskills)` `mean(education)`\n       &lt;dbl&gt;                  &lt;dbl&gt;             &lt;dbl&gt;\n1          0                  0.806              3.64\n2          1                  0.926              3.47\n\n\nWe find that indeed, those who get these jobs have on average higher computer efficiency and lower education. Since the people who go for these jobs are majority women, this would explain this phenomena of unbalanced computerskills and education across sex. This is not a problem because we only need the other variables to be balanced when divided into race. In other words, we only need all other variables to be orthogonal with the treatment, which in this case is race, not sex. We have verified this type of balancedness (across races) in the previous parts."
  },
  {
    "objectID": "labor_market_ex.html#q5",
    "href": "labor_market_ex.html#q5",
    "title": "Bias in the Labor Market",
    "section": "Q5",
    "text": "Q5\n\na)\nCalculate the average callback rate for all resumes in bm.\n\nmean(bm[[\"call\"]]) * 100\n\n[1] 8.049281\n\n\nThis corresponds to the 8.05 percent callback rate in the table.\n\n\nb)\nCalculate the average callback rates separately for resumes with “white-sounding” and “black-sounding” names. What do your results suggest?\n\nbm |&gt;\n  group_by(race)|&gt;\n  summarize(mean(call))\n\n# A tibble: 2 × 2\n  race  `mean(call)`\n  &lt;chr&gt;        &lt;dbl&gt;\n1 b           0.0645\n2 w           0.0965\n\n\nThese results corresponds to the results in the first row of table 1 in the paper, and suggests that controlling for all other factors, having a white-sounding name results in a 3% increase in callback likelihood than a black-sounding one.\n\n\nc)\nRepeat part 2, but calculate the average rates for each combination of race and sex. What do your results suggest?\n\nbm |&gt;\n  group_by(race, sex)|&gt;\n  summarize(mean(call))\n\n`summarise()` has grouped output by 'race'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 4 × 3\n# Groups:   race [2]\n  race  sex   `mean(call)`\n  &lt;chr&gt; &lt;chr&gt;        &lt;dbl&gt;\n1 b     f           0.0663\n2 b     m           0.0583\n3 w     f           0.0989\n4 w     m           0.0887\n\n\nThe results again align with the table, and seem to suggest that females on average get more callbacks than males. However, this cannot be interpreted the same way that race could causally because sex is not randomly assigned: there are some jobs where the majority of applications (that the researchers choose) were female, which means that it is not random assignment."
  },
  {
    "objectID": "labor_market_ex.html#q6",
    "href": "labor_market_ex.html#q6",
    "title": "Bias in the Labor Market",
    "section": "Q6",
    "text": "Q6\n\nTest the null hypothesis that there is no difference in callback rates across black and white-sounding names against the two-sided alternative. Comment on your results.\n\nblack_callback &lt;- pull(filter(bm, race == \"b\"), call)\nwhite_callback &lt;- pull(filter(bm, race == \"w\"), call)\nt.test(black_callback, white_callback, alternative = \"two.sided\")\n\n\n    Welch Two Sample t-test\n\ndata:  black_callback and white_callback\nt = -4.1147, df = 4711.6, p-value = 3.943e-05\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.04729503 -0.01677067\nsample estimates:\n mean of x  mean of y \n0.06447639 0.09650924 \n\n\nTherefore, since our hypothesis is that:\n\nNull Hypothesis: There is no difference in means between black and white-sounding names.\nAlternative Hypothesis: The true difference in means is not equal to 0.\n\nthese results suggest that there is a statistically significant difference in callback rates between black and white-sounding names, with white-sounding names having a higher mean callback rate than black-sounding names. In particular, there is a substantial difference between the mean of the two groups, with a p-value well below the conventional significance level of 0.05."
  },
  {
    "objectID": "repriv.html",
    "href": "repriv.html",
    "title": "Redefining Privilege",
    "section": "",
    "text": "On Shabbat, Jewish believers meet in a synagogue and each of them makes a confession. First, the rabbi says, “Oh my god I am nobody. God, I am not worthy of your attention.” Then, a rich merchant says, “God, I am also a nobody. I am not even worthy of your attention.” Finally, a peasant stands up and says, “oh God, I am also a nobody.” The rich merchant then kicks the rabbi, and says to him, “who is this guy who can also say he’s a nobody?”\nStudying at Oxford, we each know this hypocritical virtue signaling all too well. Even though we are incredibly privileged, we at times paint ourselves as victims. We compare our accommodation to those of other universities and proclaim that we are the unfortunate ones who have the small communal kitchens. We tell our friends back home that they couldn’t fathom the amount of stress and work we put up with, even if they tried.\nAnd to be frank, why wouldn’t we paint ourselves as victims? Boasting about our privileges is unattractive. It shows unawareness and undeservingness of the advantages we receive without merit. Thus, we either paint ourselves as victims or denounce our privileged identities as vices to signal self-awareness. But whether we admit it, we are all both the privileged and the unfortunate. Like many others, I can tell my story before Oxford in two ways. In one version, I spent most of my education in a Chinese public school, where classes were taught in Mandarin. To fulfill my dreams of studying overseas in higher education, I had to, from a young age, learn to read and write English outside of school without direct access to resources such as Google or YouTube. But I can equally describe my childhood as incredibly privileged. My family can afford my education overseas in Oxford and private school in America, a privilege that most of my Chinese friends lack. Both narratives are equally true and not mutually exclusive. They are both parts of who I am and no single narrative tells the whole truth.\nSince “privilege” holds such a negative connotation, we tend to hide our more privileged narratives from others. Showing a lack of privilege is even rewarded institutionally. With American universities admitting students based on “the difficulties we encounter in our life which can be fundamental to later success,” applicants are incentivized to paint themselves as victims. Due to the inability to enforce the truthfulness of these personal narratives, candidates are not being held responsible for writing skewed or plainly disingenuous narratives to outcompete other candidates’ yearnings for why they also “deserve” a spot. Even if elite institutions are responsible for creating social equity, it is hard to argue against the fact that institutionally rewarding victimhood encourages lying and hypocrisy. What’s worse,\nsetting up institutional selection processes based on victimhood sets a false standard that individuals’ privileges can be compared and ranked. In reality, it is hard to compare two completely different stories and then determine who is more deserving of what. There are countless categories, identities, and social groups, between which exist countless statistical differences. To say that some inter-group differences count as a privilege and others don’t is incorrect and even dangerous. The challenge of determining one individual’s net privilege at one given time is already a daunting task. A beautiful woman may gain many things, material and immaterial, just with her physical attractiveness. Yet when age begins to rob her of that beauty, she may be left unable to cope with the loss of her previous privilege compared with others who have never experienced the benefits of good looks. The attempt to compare privileges between different people, with each in different stages of their respective life cycles, is unrealistic and a self-defeating end.\nThe even greater danger of the modern-day definition of privilege is that it gives the self-anointed judges of privilege immense power to invalidate others who are underprivileged in categories that our society chooses to ignore.\nSure – white, rich, private-school educated students might be privileged in terms of the help they receive from their school or parents when applying to Oxford, and they should rightfully admit so. But who are we to judge whether these privileges trump all other categories of suffering? – suffering researched by psychotherapist Prof. Joy Schaverien, who documented the tendencies of emotional detachment, selective amnesia, and substance abuse in adults who went to boarding school from a young age.\nThe self-proclaimed judge of privilege chooses to ignore these disparities between private and non-private school students because it does not fit into their narrative. But making the current social narrative of privilege unfalsifiable and therefore sacred is extremely dangerous. Social narratives dictate public policy, which have great consequences that spread through society for generations.\nFor instance, the social narrative of climate change brought about carbon taxation and climate activism. Equally, the social narrative of white superiority brought about Jim Crow laws and racial segregation. Politics is indeed downstream from culture, which makes defining culturally sensitive terms like privilege all the more important.\nSocial narratives set the agenda for both thought and action. But instead of testing its accuracy against empirical evidence, narratives can be used maliciously for censorship and tyranny when enforced by judges who actively exclude views against it. These judges often resort to violence and oppression. To avoid besmirching the current social narrative of privilege, judges must also invalidate the suffering of the “privileged”. They suffer in silence as a result.\nTo become a “judge” of privilege also comes with an entrance fee. One first denounces their own privileged identities through virtue signaling – just like the rabbi and merchant – publicizing instead the underprivileged parts of their narratives. In fact, we do this every time we say “as a - insert identity -”, followed by accusations, blame, or condemnation of another’s identity. The “judge” uses the position of a self-proclaimed victim, oftentimes disingenuously so, as an instrument to justify their newfound power over others.\nThis understanding of privilege, where people use their identities to invalidate others, contradicts the fundamental purpose of privilege-checking: to understand identity groups we are not a part of.\nThe goal of checking our privileges as a social movement should be a way for more people to understand the predicaments of those around us. This goal is inherently different from one that tolerates and encourages people to compete for and judge who has more privilege based on a bundle of abstract categories – namely: white, private school educated, rich, maybe even Asian?\nThe best way of defining and understanding privilege is what you overlook in your day-to-day life that other people don’t get to. We should still listen to people who have different life experiences than ourselves. The idea of privilege should not be a tool to make others feel bad, or a quality that the “privileged” and “undeserving” could never reconcile with. We can always try to understand others’ predicaments by drawing from our personal experiences, however different or challenging it may be.\nDefined constructively, privilege helps us better understand the trials and tribulations of people who are not ourselves. It is hard to put yourself in someone else’s shoes if you don’t understand exactly what being in their shoes entails."
  },
  {
    "objectID": "football_markets_ex.html",
    "href": "football_markets_ex.html",
    "title": "College Football Rankings and Market Efficiency",
    "section": "",
    "text": "library(tidyverse)\nfootball &lt;- read_csv('https://ditraglia.com/data/fair_football.csv')\n\nRows: 1582 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (10): SPREAD, H, MAT, SAG, BIL, COL, MAS, DUN, REC, LV\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nfootball\n\n# A tibble: 1,582 × 10\n   SPREAD     H   MAT   SAG   BIL   COL   MAS   DUN   REC    LV\n    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     34     1     7    31    28    17    38    14  0     24  \n 2     29    -1    34    29    10    41    26    18 33.3   13.5\n 3     10    -1   -16   -23   -33     5   -12   -25  8.33 -10.5\n 4    -11     1     2    -8    -8    -7    -2    -4  0      3  \n 5     35    -1    35    35    38    25    25    28 25      5  \n 6     -2     1    29    36    17    25    20    11 33.3   11.5\n 7     11     1    35    39    28    40    30    34 41.7   10  \n 8     20     1    29    13    12    37    13    26 25      7.5\n 9      7     1    40    41    -7    45    36    43 66.7   11.5\n10     20    -1    61    37    36    80    51    35 75     11  \n# ℹ 1,572 more rows"
  },
  {
    "objectID": "football_markets_ex.html#q1",
    "href": "football_markets_ex.html#q1",
    "title": "College Football Rankings and Market Efficiency",
    "section": "Q1:",
    "text": "Q1:\n\nhome &lt;- football |&gt; \n  filter(H == 1) |&gt;\n  select(SPREAD)\n\navg_spread &lt;- mean(home[[\"SPREAD\"]])\nwin_rate &lt;- sum(home &gt; 0) / nrow(home) * 100\n\nc(win_rate, avg_spread)\n\n[1] 81.20482 14.95181\n\n\nThe home team wins 81% of the time, and the home team scores on average 14.95 more points than their opponent."
  },
  {
    "objectID": "football_markets_ex.html#q2",
    "href": "football_markets_ex.html#q2",
    "title": "College Football Rankings and Market Efficiency",
    "section": "Q2:",
    "text": "Q2:\n\nreg_q2 &lt;- lm(SPREAD ~ H - 1, football) \ntidy(reg_q2)\n\n# A tibble: 1 × 5\n  term  estimate std.error statistic  p.value\n  &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 H         4.86     0.537      9.04 4.29e-19\n\nglance(reg_q2)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic p.value    df logLik    AIC    BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1    0.0492        0.0486  20.7        NA      NA    NA -7035. 14074. 14084.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nThe p value is basically 0, which means that compared to a linear model along the x axis, having the term H most certainly minimizes MSE better than nothing. However, the adjusted R squared is 0.048, which means with the intercept at (0,0), only using H to predict SPREAD only explains 5 percent of it’s variance. It does not make sense to include a constant in regression to predict spread because the numerical value of SPREAD is not meaningful as the teams are randomly chosen. SPREAD is only meaningful when its differences are compared given the change of an independent variable, as a proxy for performance."
  },
  {
    "objectID": "football_markets_ex.html#q3",
    "href": "football_markets_ex.html#q3",
    "title": "College Football Rankings and Market Efficiency",
    "section": "Q3:",
    "text": "Q3:\n\nlibrary(GGally)\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\nfootball_rank &lt;- football|&gt;\n  select(-SPREAD, -H, -LV)\nggpairs(football_rank)\n\n\n\n\nThe scattorplots below the diagonal of the matrix helps visualize the relationships between variables, the diagonal displays histograms for each variable, and above the diagonal displays the correlations between each 2 variables. We see that the different ranking systems are all highly correlated with one another, and are all basically centered around 0. This makes intuitive sense as the metric is a difference of rank, and that they are all constructed based on the team’s REC."
  },
  {
    "objectID": "football_markets_ex.html#q4",
    "href": "football_markets_ex.html#q4",
    "title": "College Football Rankings and Market Efficiency",
    "section": "Q4:",
    "text": "Q4:\n\nreg_q4 &lt;- lm(SPREAD ~ H + REC + MAT + SAG + BIL + COL + MAS + DUN - 1, football)\ntidy(reg_q4)\n\n# A tibble: 8 × 5\n  term  estimate std.error statistic  p.value\n  &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 H      4.27       0.437      9.77  6.16e-22\n2 REC    0.0804     0.0305     2.64  8.37e- 3\n3 MAT   -0.0993     0.0608    -1.63  1.03e- 1\n4 SAG    0.248      0.0548     4.53  6.43e- 6\n5 BIL    0.0804     0.0342     2.35  1.90e- 2\n6 COL   -0.0626     0.0359    -1.74  8.14e- 2\n7 MAS   -0.00707    0.0446    -0.159 8.74e- 1\n8 DUN    0.119      0.0338     3.51  4.62e- 4\n\nglance(reg_q4)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic   p.value    df logLik    AIC    BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1     0.394         0.391  16.5      128. 2.40e-165     8 -6678. 13374. 13423.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nMAT, COL, and MAS does not pass the 95% test, as their p-values are all larger than 0.05. This means that there is not enough evidence to reject the null hypothesis that the coefficient is equal to zero in these predictors. Now we drop these three predictors:\n\nreg_q4_2 &lt;- lm(SPREAD ~ H + REC + SAG + DUN + BIL - 1, football)\ntidy(reg_q4_2)\n\n# A tibble: 5 × 5\n  term  estimate std.error statistic  p.value\n  &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 H       4.40      0.435      10.1  2.17e-23\n2 REC     0.0267    0.0234      1.14 2.54e- 1\n3 SAG     0.144     0.0345      4.18 3.14e- 5\n4 DUN     0.116     0.0314      3.71 2.17e- 4\n5 BIL     0.0610    0.0337      1.81 7.00e- 2\n\nglance(reg_q4_2)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic   p.value    df logLik    AIC    BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1     0.391         0.389  16.6      202. 1.12e-166     5 -6683. 13378. 13410.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nDropping BIL and REC:\n\nreg_q4_3 &lt;- lm(SPREAD ~ H + SAG + DUN - 1, football)\ntidy(reg_q4_3)\n\n# A tibble: 3 × 5\n  term  estimate std.error statistic  p.value\n  &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 H        4.37     0.434      10.1  3.66e-23\n2 SAG      0.191    0.0277      6.88 8.65e-12\n3 DUN      0.143    0.0283      5.05 5.06e- 7\n\nglance(reg_q4_3) |&gt; \n  mutate(Row_Names = c(\"H + SAG + DUN\")) |&gt;\n  column_to_rownames(var = \"Row_Names\")\n\n              r.squared adj.r.squared   sigma statistic       p.value df\nH + SAG + DUN 0.3883864     0.3872244 16.5793  334.2318 5.249385e-168  3\n                 logLik      AIC      BIC deviance df.residual nobs\nH + SAG + DUN -6685.761 13379.52 13400.99 434024.9        1579 1582\n\n\nThis seems to suggest that SAG and DUN provides new information outside of accounting for the home-advantage, and also that in the presence of SAG and DUN, the home-field advanateg still has additional predictive power in SPREAD. Now let’s compare the adjusted R squared value of this regression with three parameters with the best of seven computer systems, which may be SAG or DUN. We test each one individually:\n\nreg_q4_4 &lt;- lm(SPREAD ~ DUN - 1, football)\nreg_q4_5 &lt;- lm(SPREAD ~ SAG - 1, football)\nrbind(tidy(reg_q4_4), tidy(reg_q4_5))\n\n# A tibble: 2 × 5\n  term  estimate std.error statistic   p.value\n  &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 DUN      0.322    0.0117      27.5 3.45e-136\n2 SAG      0.326    0.0113      28.7 2.07e-146\n\nrbind(glance(reg_q4_4), glance(reg_q4_5)) |&gt;\n  mutate(Row_Names = c(\"DUN\", \"SAG\")) |&gt;\n  column_to_rownames(var = \"Row_Names\")\n\n    r.squared adj.r.squared    sigma statistic p.value df    logLik      AIC\nDUN 0.3231763     0.3227482 17.42973        NA      NA NA -6765.897 13535.79\nSAG 0.3430071     0.3425915 17.17249        NA      NA NA -6742.374 13488.75\n         BIC deviance df.residual nobs\nDUN 13546.53 480300.5        1581 1582\nSAG 13499.48 466227.8        1581 1582\n\n\nIt turns out that neither of these have a larger adjusted r squared as large as the regression where we take into account H and combine these two predictors."
  },
  {
    "objectID": "football_markets_ex.html#q5",
    "href": "football_markets_ex.html#q5",
    "title": "College Football Rankings and Market Efficiency",
    "section": "Q5:",
    "text": "Q5:\n\nreg_q5 &lt;- lm(SPREAD ~ LV + H + DUN + SAG - 1, football)\ntidy(reg_q5)\n\n# A tibble: 4 × 5\n  term  estimate std.error statistic  p.value\n  &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 LV      1.04      0.0731    14.3   1.19e-43\n2 H       0.743     0.481      1.54  1.23e- 1\n3 DUN    -0.0344    0.0293    -1.17  2.41e- 1\n4 SAG     0.0151    0.0289     0.522 6.02e- 1\n\nglance(reg_q5)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic   p.value    df logLik    AIC    BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1     0.458         0.457  15.6      334. 2.49e-208     4 -6590. 13189. 13216.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nNope, when controlling for LV, no other ranking systems, or H contain additional predictive information beyond that contained in LV. As the p values in H, DUN, and SAG are all quite large, and so we cannot reject the null of it’s coefficient being 0 when predicting SPREAD (has no additional predictive power)."
  },
  {
    "objectID": "football_markets_ex.html#q6",
    "href": "football_markets_ex.html#q6",
    "title": "College Football Rankings and Market Efficiency",
    "section": "Q6:",
    "text": "Q6:\nThe Efficient Market Hypothesis posits that betting (financial) markets are efficient in processing all relevant information, making it impossible to consistently outperform the market by using past or publically available information. The findings in the previous part has to do with market efficiency as there is no additional information that can predict game results better than the betting market, which is consistent with this theory.\nIf bettering markets are efficient, the slope in a regression that uses LV alone to predict SPREAD should be one. As in this case the equilibrium betting spread would be the best predictor of the actual SPREAD which incorporates information from all other forms of available information (measured in this dataset). Let’s run a regression and find its confidence interval.\n\nreg_q6 &lt;- lm(SPREAD ~ LV - 1, football)\ntidy(reg_q6)\n\n# A tibble: 1 × 5\n  term  estimate std.error statistic   p.value\n  &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 LV        1.01    0.0279      36.4 2.09e-211\n\nglance(reg_q6)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic p.value    df logLik    AIC    BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1     0.456         0.456  15.6        NA      NA    NA -6593. 13190. 13200.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\nconfint(reg_q6)\n\n       2.5 %   97.5 %\nLV 0.9597332 1.068992\n\n\nI cannot statistically reject the value of 1 as the true coefficient of LV as there is a 95% chance that the CI of the regression contains the true population parameter (1), using only LV predicting SPREAD. Accuracy wise: the adjusted R squared is about 0.46, which means LV alone explains about 46 percent of the variance in SPREAD, this means LV explains a large portion in the variability in spread.\n\npredictions &lt;- predict(reg_q6)\nactual_values &lt;- football$SPREAD\nrmse &lt;- sqrt(mean((actual_values - predictions)^2))\nprint(paste(\"RMSE of LV:\", rmse))\n\n[1] \"RMSE of LV: 15.6179667671242\"\n\npredictions_5 &lt;- predict(reg_q5)\nactual_values_5 &lt;- football$SPREAD\nrmse_5 &lt;- sqrt(mean((actual_values_5 - predictions_5)^2))\nprint(paste(\"RMSE of best reg w/o/ LV:\", rmse_5))\n\n[1] \"RMSE of best reg w/o/ LV: 15.5859031070086\"\n\n\nCalculating the RMSE of using only LV and using H plus the other two useful predictors, we find that the RMSE of only LV is slightly smaller than the regression from the previous regression in part 5. Combine this with with a higher adjusted R squared, it suggests that only using LV is both more accurate and could explain more of the data in SPREAD than using the best combination of the other predictors."
  },
  {
    "objectID": "football_markets_ex.html#q7",
    "href": "football_markets_ex.html#q7",
    "title": "College Football Rankings and Market Efficiency",
    "section": "Q7:",
    "text": "Q7:\n\ncomb_reg &lt;- list(reg_q2, reg_q4, reg_q4_2, reg_q4_3, reg_q4_4, reg_q4_5, reg_q5, reg_q6)\nmodelsummary(comb_reg, gof_omit = 'Log.Lik|AIC|BIC|F', fmt = 2)\n\n\n\n\n\n (1)\n  (2)\n  (3)\n  (4)\n  (5)\n  (6)\n  (7)\n  (8)\n\n\n\n\nH\n4.86\n4.27\n4.40\n4.37\n\n\n0.74\n\n\n\n\n(0.54)\n(0.44)\n(0.43)\n(0.43)\n\n\n(0.48)\n\n\n\nREC\n\n0.08\n0.03\n\n\n\n\n\n\n\n\n\n(0.03)\n(0.02)\n\n\n\n\n\n\n\nMAT\n\n−0.10\n\n\n\n\n\n\n\n\n\n\n(0.06)\n\n\n\n\n\n\n\n\nSAG\n\n0.25\n0.14\n0.19\n\n0.33\n0.02\n\n\n\n\n\n(0.05)\n(0.03)\n(0.03)\n\n(0.01)\n(0.03)\n\n\n\nBIL\n\n0.08\n0.06\n\n\n\n\n\n\n\n\n\n(0.03)\n(0.03)\n\n\n\n\n\n\n\nCOL\n\n−0.06\n\n\n\n\n\n\n\n\n\n\n(0.04)\n\n\n\n\n\n\n\n\nMAS\n\n−0.01\n\n\n\n\n\n\n\n\n\n\n(0.04)\n\n\n\n\n\n\n\n\nDUN\n\n0.12\n0.12\n0.14\n0.32\n\n−0.03\n\n\n\n\n\n(0.03)\n(0.03)\n(0.03)\n(0.01)\n\n(0.03)\n\n\n\nLV\n\n\n\n\n\n\n1.04\n1.01\n\n\n\n\n\n\n\n\n\n(0.07)\n(0.03)\n\n\nNum.Obs.\n1582\n1582\n1582\n1582\n1582\n1582\n1582\n1582\n\n\nR2\n0.049\n0.394\n0.391\n0.388\n0.323\n0.343\n0.458\n0.456\n\n\nR2 Adj.\n0.049\n0.391\n0.389\n0.387\n0.323\n0.343\n0.457\n0.456\n\n\nRMSE\n20.65\n16.48\n16.53\n16.56\n17.42\n17.17\n15.59\n15.62"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Monte Carlo On a Desert Island",
    "section": "",
    "text": "runif_zx81 &lt;- function(seed, n, min, max) {\n  # Setting the parameters and initial seed\n  m &lt;- 2^16 + 1\n  a &lt;- 75\n  x &lt;- seed\n\n  # Generating an empty vector\n  random_numbers &lt;- numeric(n)\n\n  # Writing a for loop to calculate each value using the Lehmer formula, scaling the value, then storing it in the random_numbers matrix\n  for (i in 1:n) {\n    x &lt;- (a * x) %% m  \n    x_scale &lt;- min + (max - min) * (x / m)  \n    random_numbers[i] &lt;- x_scale\n  }\n  \n  random_numbers\n}\n\n\n\n\nTesting for this runif generator\n\nunif_sim &lt;- runif_zx81(69, 1000, 0, 10)\nmean(unif_sim)\n\n[1] 4.907436\n\n\nOn first glance just by taking the mean it seems like this would apprach the mean for a similar uniform distribution. Making a histogram:\n\ntibble(unif_sim) |&gt;\n  ggplot(aes(x = unif_sim)) +\n  geom_histogram(bins = 30) +\n  labs(x = \"Values of Draws\", y = \"Number of occurances\", title = \"Histogram of runif_ZX81 Random Draws\")\n\n\n\n\nThis seems reasonably uniformly distributed except the end points, take less values than the other values. Now making a qqplot:\n\ntibble(x = unif_sim) |&gt;\n  ggplot(aes(sample = x)) +\n  geom_qq(distribution = qunif)+\n  geom_qq_line(col = \"red\", distribution = qunif) +\n  geom_abline(slope = 1, intercept = 0, col = \"blue\") +\n  labs(title = \"QQ Plot of runif_ZX81 Random Draws\")\n\n\n\n\nThis simulation has the same distribution as the uniform distribution as they are on the same line (red line), which means the quantiles of the observed data agree with those of a transformed uniform distribution. If we standardize the data before making such a plot (making the min and max (0,1)), then we get a 45 degree line (blue line):\n\nunif_sim_sd &lt;- runif_zx81(69, 1000, 0, 1)\ntibble(x = unif_sim_sd) |&gt;\n  ggplot(aes(sample = x)) +\n  geom_qq(distribution = qunif)+\n  geom_qq_line(col = \"red\", distribution = qunif) +\n  geom_abline(slope = 1, intercept = 0, col = \"blue\") +\n  labs(title = \"Normalized QQ Plot of runif_ZX81 Random Draws\")\n\n\n\n\nNow making the time-series plot:\n\ntibble(Index = 1:1000, Value = unif_sim) |&gt;\nggplot(aes(x = Index, y = Value)) +\n  geom_line() +\n  labs(x = \"Index of Draw\", y = \"Value of Draw\", title = \"Time Series Plot of runif_ZX81 Random Draws\")\n\n\n\n\nThere does not seem to be any clustering or gaps, the variance around 5 all seem constant and pure noise without much persistence."
  },
  {
    "objectID": "projects.html#q1",
    "href": "projects.html#q1",
    "title": "Monte Carlo On a Desert Island",
    "section": "",
    "text": "runif_zx81 &lt;- function(seed, n, min, max) {\n  # Setting the parameters and initial seed\n  m &lt;- 2^16 + 1\n  a &lt;- 75\n  x &lt;- seed\n\n  # Generating an empty vector\n  random_numbers &lt;- numeric(n)\n\n  # Writing a for loop to calculate each value using the Lehmer formula, scaling the value, then storing it in the random_numbers matrix\n  for (i in 1:n) {\n    x &lt;- (a * x) %% m  \n    x_scale &lt;- min + (max - min) * (x / m)  \n    random_numbers[i] &lt;- x_scale\n  }\n  \n  random_numbers\n}\n\n\n\n\nTesting for this runif generator\n\nunif_sim &lt;- runif_zx81(69, 1000, 0, 10)\nmean(unif_sim)\n\n[1] 4.907436\n\n\nOn first glance just by taking the mean it seems like this would apprach the mean for a similar uniform distribution. Making a histogram:\n\ntibble(unif_sim) |&gt;\n  ggplot(aes(x = unif_sim)) +\n  geom_histogram(bins = 30) +\n  labs(x = \"Values of Draws\", y = \"Number of occurances\", title = \"Histogram of runif_ZX81 Random Draws\")\n\n\n\n\nThis seems reasonably uniformly distributed except the end points, take less values than the other values. Now making a qqplot:\n\ntibble(x = unif_sim) |&gt;\n  ggplot(aes(sample = x)) +\n  geom_qq(distribution = qunif)+\n  geom_qq_line(col = \"red\", distribution = qunif) +\n  geom_abline(slope = 1, intercept = 0, col = \"blue\") +\n  labs(title = \"QQ Plot of runif_ZX81 Random Draws\")\n\n\n\n\nThis simulation has the same distribution as the uniform distribution as they are on the same line (red line), which means the quantiles of the observed data agree with those of a transformed uniform distribution. If we standardize the data before making such a plot (making the min and max (0,1)), then we get a 45 degree line (blue line):\n\nunif_sim_sd &lt;- runif_zx81(69, 1000, 0, 1)\ntibble(x = unif_sim_sd) |&gt;\n  ggplot(aes(sample = x)) +\n  geom_qq(distribution = qunif)+\n  geom_qq_line(col = \"red\", distribution = qunif) +\n  geom_abline(slope = 1, intercept = 0, col = \"blue\") +\n  labs(title = \"Normalized QQ Plot of runif_ZX81 Random Draws\")\n\n\n\n\nNow making the time-series plot:\n\ntibble(Index = 1:1000, Value = unif_sim) |&gt;\nggplot(aes(x = Index, y = Value)) +\n  geom_line() +\n  labs(x = \"Index of Draw\", y = \"Value of Draw\", title = \"Time Series Plot of runif_ZX81 Random Draws\")\n\n\n\n\nThere does not seem to be any clustering or gaps, the variance around 5 all seem constant and pure noise without much persistence."
  },
  {
    "objectID": "projects.html#q2",
    "href": "projects.html#q2",
    "title": "Monte Carlo On a Desert Island",
    "section": "Q2",
    "text": "Q2\n\na)\nWe first need to create two independent uniform RV from two different seeds to make sure that they are indepdnent of one another. This is becasue the same seed number affects the sequence of psedo-random numbers generated by the ZX81 random number generator. We end up just having to use one of the two standard normal simulations:\n\nrnorm_zx81 &lt;- function(seed, n, mean, sd) {\n  u1 &lt;- runif_zx81(seed +1, n, 0, 1)\n  u2 &lt;- runif_zx81(seed, n, 0, 1)\n  R &lt;- sqrt(-2 * log(u1))\n  theta &lt;- 2 * pi * u2\n  z1 &lt;- R * cos(theta) \n  z1\n}\nrnorm_zx81(69, 10, 0, 1)\n\n [1]  1.9760262  2.7418138  0.4998739 -1.4911231 -0.5213273  1.5138666\n [7]  1.8235343  0.1026315  0.8463807  0.3671726\n\n\n\n\nb)\n\nnorm_sim &lt;- rnorm_zx81(69, 1000, 0, 1)\nmean(norm_sim)\n\n[1] 0.00163546\n\nsd(norm_sim)\n\n[1] 0.9741741\n\n\nAt first glance the mean and standard deviation seems reasonable given a large sample size. Let’s make some plots similar to the first question of this worksheet.\n\ntibble(norm_sim) |&gt;\n  ggplot(aes(x = norm_sim)) +\n  geom_histogram(bins = 30) +\n  labs(x = \"Values of the Simulation\", y = \"Number of Draws\", title = \"Histogram of rnorm_ZX81 Random Draws\")\n\n\n\n\nNote to self: it is very frustrating that I do not know how to overlay a normally distributed curve on this histogram (nothing seems to work)\n\ntibble(x = norm_sim) |&gt;\n  ggplot(aes(sample = x)) +\n  geom_qq()+\n  geom_abline(slope = 1, intercept = 0, col = \"blue\") +\n  labs(title = \"QQ Plot of rnorm_ZX81 Random Draws\")\n\n\n\n\nOverall this lines up with the 45 degree line well for values of x from [-2,2].\n\ntibble(Index = 1:1000, Value = norm_sim) |&gt;\nggplot(aes(x = Index, y = Value)) +\n  geom_line() +\n  labs(x = \"Index of Draw\", y = \"Value of Draw\", title = \"Time Series Plot of rnorm_ZX81 Random Draws\")\n\n\n\n\nOverall this time series plot centers around 0, has constant variance and at no point are the values persistently higher or lower than 0."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "Hello! My name is Toby and I am a recent graduate of the University of Oxford where I studied Philosophy, Politics, and Economics (PPE). Currently, I am a masters student at Yale University’s School of Management studying for a Masters in Asset Management, a program created by PMs at AQR Capital and David Swenson, the ex Chief Investment Officer of the Yale Endowment. This website is meant to show my writing, both academic and non-academic, as well as the various data projects I am excited to share with the world! Stay tuned for updates to come!\n:::\nThanks for checking out my Website!"
  },
  {
    "objectID": "optimal_stop_ex.html",
    "href": "optimal_stop_ex.html",
    "title": "Optimal Stopping",
    "section": "",
    "text": "The basic thought process behind this version of the simulation setup is that:\n\nRandomly sample the ranks of the phase 1 supervisors\nOrder the list of supervisors from best score to worst score, and select the one from the first value of that vector, as well as his rank (test_list_best)\nThen sample the supervisors in phase 2 by randomly assiging each supervior with a rank, exclusing the ranks that have been given to the phase 1 supervisors (super_list_rank)\nFinally, cycle through each supervior, checking if their rank is higher than the largest out of all professors in phase 1, returning the rank of the processor I end up with.\n\n\nset.seed(69)\nk &lt;- 40\nn &lt;- 50\nsupervisor_sims &lt;- \\(n, k) {\n  test_list &lt;- c(1:k)\n  test_list_rank &lt;- sample(1:n, k, replace = FALSE)\n  test_list_ordered &lt;- test_list[order(test_list_rank, decreasing = TRUE)]\n  test_list_best &lt;- c(test_list_ordered[1], max(test_list_rank))\n  super_list &lt;- c(k+1, n)\n  super_list_rank &lt;- sample(1:n, n, replace = FALSE)\n  super &lt;- which(!super_list_rank %in% test_list_rank)\n  super_list_rank &lt;- super_list_rank[super]\n  i &lt;- 1\n  while (super_list_rank[i] &gt; min(test_list_rank) & i &lt; n-k) {\n    i &lt;- i+1\n  }\n  super_list_rank[i]\n}\nset.seed(69)\nmean(map_dbl(1:100000, \\(i) supervisor_sims(50, 40)))\n\n[1] 20.95339\n\n\n\n\n\nEven though the result on the first is correct (as it matched with the code below when sampling them many times), I found a simplier way to produce the same output. The basic idea is that we sample all the superviors rank in one go (this is probably the easiest way to do it).\n\nset.seed(69)\nsupervisor_sim &lt;- \\(n, k) {\n  # Randomly generate scores for n supervisors\n  supervisors_rank &lt;- sample(1:n, n, replace = FALSE)\n  \n  # Sample k supervisors for phase 1\n  phase_one_rank &lt;- supervisors_rank[1:k]\n  min_rank &lt;- min(phase_one_rank)\n  \n  # Start phase 2\n  for (i in (k+1):n) {\n    if (supervisors_rank[i] &lt; min_rank) {\n      return(supervisors_rank[i])\n    }\n  }\n  \n  # If no supervisor found in phase 2, return last one\n  return(supervisors_rank[n])\n}\n\nmean(map_dbl(1:100000, \\(i) supervisor_sim(50, 40)))\n\n[1] 21.02788"
  },
  {
    "objectID": "optimal_stop_ex.html#q1",
    "href": "optimal_stop_ex.html#q1",
    "title": "Optimal Stopping",
    "section": "",
    "text": "The basic thought process behind this version of the simulation setup is that:\n\nRandomly sample the ranks of the phase 1 supervisors\nOrder the list of supervisors from best score to worst score, and select the one from the first value of that vector, as well as his rank (test_list_best)\nThen sample the supervisors in phase 2 by randomly assiging each supervior with a rank, exclusing the ranks that have been given to the phase 1 supervisors (super_list_rank)\nFinally, cycle through each supervior, checking if their rank is higher than the largest out of all professors in phase 1, returning the rank of the processor I end up with.\n\n\nset.seed(69)\nk &lt;- 40\nn &lt;- 50\nsupervisor_sims &lt;- \\(n, k) {\n  test_list &lt;- c(1:k)\n  test_list_rank &lt;- sample(1:n, k, replace = FALSE)\n  test_list_ordered &lt;- test_list[order(test_list_rank, decreasing = TRUE)]\n  test_list_best &lt;- c(test_list_ordered[1], max(test_list_rank))\n  super_list &lt;- c(k+1, n)\n  super_list_rank &lt;- sample(1:n, n, replace = FALSE)\n  super &lt;- which(!super_list_rank %in% test_list_rank)\n  super_list_rank &lt;- super_list_rank[super]\n  i &lt;- 1\n  while (super_list_rank[i] &gt; min(test_list_rank) & i &lt; n-k) {\n    i &lt;- i+1\n  }\n  super_list_rank[i]\n}\nset.seed(69)\nmean(map_dbl(1:100000, \\(i) supervisor_sims(50, 40)))\n\n[1] 20.95339\n\n\n\n\n\nEven though the result on the first is correct (as it matched with the code below when sampling them many times), I found a simplier way to produce the same output. The basic idea is that we sample all the superviors rank in one go (this is probably the easiest way to do it).\n\nset.seed(69)\nsupervisor_sim &lt;- \\(n, k) {\n  # Randomly generate scores for n supervisors\n  supervisors_rank &lt;- sample(1:n, n, replace = FALSE)\n  \n  # Sample k supervisors for phase 1\n  phase_one_rank &lt;- supervisors_rank[1:k]\n  min_rank &lt;- min(phase_one_rank)\n  \n  # Start phase 2\n  for (i in (k+1):n) {\n    if (supervisors_rank[i] &lt; min_rank) {\n      return(supervisors_rank[i])\n    }\n  }\n  \n  # If no supervisor found in phase 2, return last one\n  return(supervisors_rank[n])\n}\n\nmean(map_dbl(1:100000, \\(i) supervisor_sim(50, 40)))\n\n[1] 21.02788"
  },
  {
    "objectID": "optimal_stop_ex.html#q2",
    "href": "optimal_stop_ex.html#q2",
    "title": "Optimal Stopping",
    "section": "Q2",
    "text": "Q2\nTo do this we just use the map function to sample lots of times and then calculate what percentage of those is ranked at number one\n\nset.seed(6969)\nget_prob_preferred &lt;- \\(n, k, sample_size = 10000) {\n  samples &lt;- map_dbl(1:sample_size, \\(i) supervisor_sim(n, k))\n  sum(samples == 1) / sample_size\n}\nget_prob_preferred(50, 5)\n\n[1] 0.2422\n\n\nJust to check that the first simulation works as well:\n\nset.seed(6969)\nget_prob_preferredd &lt;- \\(n, k, sample_size = 10000) {\n  samples &lt;- map_dbl(1:sample_size, \\(i) supervisor_sims(n, k))\n  sum(samples == 1) / sample_size\n}\nget_prob_preferredd(50, 5)\n\n[1] 0.2399"
  },
  {
    "objectID": "optimal_stop_ex.html#q3",
    "href": "optimal_stop_ex.html#q3",
    "title": "Optimal Stopping",
    "section": "Q3",
    "text": "Q3\n\nset.seed(6969)\nk_50 &lt;- c(5:25)\n\none_map &lt;- \\(k_50, sample_size){\n   map_dbl(k_50, \\(k_50) get_prob_preferred(50, k_50, sample_size))\n}\nn &lt;- 5\nsample_seq &lt;- seq(from = 1e4, by = 5e3, length.out = n)\nnames(sample_seq) &lt;- paste0(\"sample_size:\", seq(from = 1e5, by = 5e4, length.out = n))\n\ndf &lt;- map_dfc(sample_seq, \\(sample_seq) one_map(k_50, sample_seq)) |&gt; \n  mutate(k = c(5:25)) |&gt;\n  pivot_longer(-k, names_to = \"sample\", values_to = \"value\")\n\ndf |&gt;\n  ggplot(aes(x = k, y = value, color = sample)) +\n  geom_line()\n\n\n\n\nOkay… Let’s now look at the 15-23 range…\n\nset.seed(6969)\nk_50 &lt;- c(15:23)\n\none_map &lt;- \\(k_50, sample_size){\n   map_dbl(k_50, \\(k_50) get_prob_preferred(50, k_50, sample_size))\n}\nn &lt;- 5\nsample_seq &lt;- seq(from = 1e5, by = 5e4, length.out = n)\nnames(sample_seq) &lt;- paste0(\"sample_size:\", seq(from = 1e5, by = 5e4, length.out = n))\n\nmap_dfc(sample_seq, \\(sample_seq) one_map(k_50, sample_seq)) |&gt; \n  mutate(k = c(15:23)) |&gt;\n  pivot_longer(-k, names_to = \"sample\", values_to = \"value\") |&gt;\n  ggplot(aes(x = k, y = value, color = sample)) +\n  geom_line()\n\n\n\n\nNow let’s narrow this down further, say 16-21\n\nset.seed(6969)\nk_50 &lt;- c(17:20)\n\none_map &lt;- \\(k_50, sample_size){\n   map_dbl(k_50, \\(k_50) get_prob_preferred(50, k_50, sample_size))\n}\nn &lt;- 5\nsample_seq &lt;- seq(from = 2e5, by = 1e5, length.out = n)\nnames(sample_seq) &lt;- paste0(\"sample_size:\", seq(from = 2e5, by = 1e5, length.out = n))\n\nmap_dfc(sample_seq, \\(sample_seq) one_map(k_50, sample_seq)) |&gt; \n  mutate(k = c(17:20)) |&gt;\n  pivot_longer(-k, names_to = \"sample\", values_to = \"value\") |&gt;\n  ggplot(aes(x = k, y = value, color = sample)) +\n  geom_line()\n\n\n\n\nIt’s hard to say for sure, but it seems as if the optimal value (the number of supervisors when n=50 at phase one) is either at 18 or 19. I’m not sure if there’s a value k that the simulation would converge on and in any case it took really long to run."
  },
  {
    "objectID": "colonial_origins_ex.html",
    "href": "colonial_origins_ex.html",
    "title": "The Colonial Origins of Comparative Development",
    "section": "",
    "text": "library(haven)\ndta &lt;- read_dta(\"https://ditraglia.com/data/ajr.dta\")\nhead(dta)\n\n# A tibble: 6 × 14\n  longname   shortnam   mort logmort0  risk loggdp latitude neoeuro  asia africa\n  &lt;chr&gt;      &lt;chr&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 Angola     AGO      280        5.63  5.36   7.77    0.137       0     0      1\n2 Argentina  ARG       68.9      4.23  6.39   9.13    0.378       0     0      0\n3 Australia  AUS        8.55     2.15  9.32   9.90    0.300       1     0      0\n4 Burkina F… BFA      280        5.63  4.45   6.85    0.144       0     0      1\n5 Bangladesh BGD       71.4      4.27  5.14   6.88    0.267       0     1      0\n6 Bolivia    BOL       71        4.26  5.64   7.93    0.189       0     0      0\n# ℹ 4 more variables: other &lt;dbl&gt;, rainmin &lt;dbl&gt;, meantemp &lt;dbl&gt;, malaria &lt;dbl&gt;"
  },
  {
    "objectID": "colonial_origins_ex.html#q1",
    "href": "colonial_origins_ex.html#q1",
    "title": "The Colonial Origins of Comparative Development",
    "section": "Q1",
    "text": "Q1\n\na.\nThe key question that AJR try to answer is why some countries are much richer than others today. Specifically, they investigate the role that institutions play in determining the level of development of various countries and argue that the differences in institutions across countries are the fundamental cause of differences in economic development.\n\n\nb.\nAJR’s key theory is that the institutions in a society are a major determinant of economic development. They argue that societies that have “good” institutions - those that provide security of property rights and relatively equal access to economic opportunities - will have better economic performance than societies with “bad” institutions - those that do not provide these things. They also argue that the colonizers established different types of institutions in different colonies, which led to differences in economic development. Specifically, in places where the disease environment was not suitable for European settlement, colonizers set up extractive institutions, which did not provide secure property rights and did not encourage investment. In places where Europeans settled in large numbers, they set up institutions that were more conducive to investment and economic development.\n\n\nc.\nExogeneity: in this context this would mean that settler mortality rates are uncorrelated with any other factors that affect economic development other than through their effect on early institutions which affects later institutions, which includes them directly affecting economic performance (exclusion). This cannot be tested because we do now know what the error term is (as it could mean anything that causes economic development except institutions). AJR argue that this holds because the factors which caused settlers mortaility was determined by the disease (malaria and yellow fever) environment under which the settlers faced, which the locals have a built immunity for (till this day). Therefore these diseases are therefore unlikely to be the reason why many countries in Africa and Asia are very poor today.\nRelavence: The instrument they use is the mortality rates of settlers. The relevance assumption here would mean that settler mortality rates are correlated with the institutions that were established by the colonizers. This can be tested and AJR claims in their paper that mortality rates faced by the settlers more than 100 years ago explains over 25 percent of the variation in current institutions."
  },
  {
    "objectID": "colonial_origins_ex.html#q2-ols-regression",
    "href": "colonial_origins_ex.html#q2-ols-regression",
    "title": "The Colonial Origins of Comparative Development",
    "section": "Q2: OLS Regression",
    "text": "Q2: OLS Regression\n\na.\n\nols &lt;- lm(loggdp ~ risk, dta)\n\n\n\nb.\n\ntidy(ols) |&gt;\n  knitr::kable(digits = 2, caption = \"OLS\")\n\n\nOLS\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n4.73\n0.41\n11.41\n0\n\n\nrisk\n0.51\n0.06\n8.11\n0\n\n\n\n\n\n\n\nc.\nwe cannot interpret these results causally because there may be variables, such as geographical location, which causes both risk and loggdp. The natural resource curse is a good illustration of this whereas it could both cause more extractive institutions, and higher gdp. Additionally, there may be reverse causation in that it may actually be loggdp causing better institutions in line with theories which state that a country would be more democratic and have better property rights once there is a growing middle class which wants their property more secure and protected."
  },
  {
    "objectID": "colonial_origins_ex.html#q3-iv-regression",
    "href": "colonial_origins_ex.html#q3-iv-regression",
    "title": "The Colonial Origins of Comparative Development",
    "section": "Q3: IV Regression",
    "text": "Q3: IV Regression\n\na. First Stage\n\nfirst_stage &lt;- lm(risk ~ logmort0, dta)\nfirst_stage |&gt;\n  tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    9.39      0.633     14.8  1.00e-21\n2 logmort0      -0.620     0.131     -4.74 1.37e- 5\n\n\nThere is a very significant relationship between settlers mortality rates and the risk of expropriation. Specifically, one percent change in settlers mortality rates is correalted with a .6 point reduction (out of 10) in how protected one’s property is.\n\n\nb. Reduced Form\n\nreduced_form &lt;- lm(loggdp ~ logmort0, dta)\nreduced_form |&gt;\n  tidy() |&gt;\n  knitr::kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n10.6336150\n0.3822061\n27.821675\n0\n\n\nlogmort0\n-0.5608809\n0.0789457\n-7.104643\n0\n\n\n\n\n\nAgain there is a statistically significant relationship between logmort0 and loggdp, indicating that a 1 percent increase in settler mortality rates is associated with a 0.56 percent decrease in GDP.\n\n\nc.\n\niv &lt;- AER::ivreg(loggdp ~ risk | logmort0, data = dta)\n\n\n\nd.\n\niv_ols &lt;- list(iv_reg = iv, ols_reg = ols)\nmodelsummary(iv_ols, gof_omit = 'Log.Lik.|R2 Adj.|AIC|BIC|F', fmt = 2)\n\n\n\n\n\niv_reg\nols_reg\n\n\n\n\n(Intercept)\n2.13\n4.73\n\n\n\n(1.01)\n(0.41)\n\n\nrisk\n0.91\n0.51\n\n\n\n(0.16)\n(0.06)\n\n\nNum.Obs.\n62\n62\n\n\nR2\n0.195\n0.523\n\n\nRMSE\n0.92\n0.71\n\n\n\n\n\n\n\nThe IV regression gives a larger effect of a unit increase of risk (better institutions) on the percentage increase of gdp in 1995. This results strengthens AJR’s argument that institutions matter in economic development, when holding other factors equal. They show this argument through this IV result which show that confounding variables (endogenous) bias the true causal result downwards.\n\n\ne.\nSince we know that given that the instrument satisfies relevance, the exclusion restriction, and independence, the IV coefficient is the reduced form coefficient over the first stage coefficient, we have:\n\niv_hand &lt;- coef(reduced_form)[2] / coef(first_stage)[2]\nround(iv_hand, digits = 2)\n\nlogmort0 \n    0.91 \n\n\nThis is the same result we got for the IV coefficient rounded to 2 decimal points."
  },
  {
    "objectID": "colonial_origins_ex.html#q4-2-criticisms-of-ajr",
    "href": "colonial_origins_ex.html#q4-2-criticisms-of-ajr",
    "title": "The Colonial Origins of Comparative Development",
    "section": "Q4: 2 Criticisms of AJR",
    "text": "Q4: 2 Criticisms of AJR\n\na.\nPutting claims 1 and 2 together we can argue that the instrument logmort0 violates the exclusion restriction, which is included in the exogeneity assumption. Claim 2 states that settler mortality rates causes a country’s disease environment today, and claim 1 states that the current disease environment determines GDP per capita. Taken together this questions if institutions is the only bridge between settlers moratality and GDP per capita.\n\n\nb.\nThis is because when we include malaria (today) as an additional regressor, we control for the current disease enviornment (or we pull it out from the error term in the causal model) when measuring the effect of institutions on GDP. By doing so claim 1 and 2 cannot hold because the current disease enviornment is no longer a part of the error term. Thus, if after controlling for malaria (current disease enviornment) we do not see a change in the IV results, this would mean that the claims (1 and 2) are invalid, and we do see a change in IV results, our IV estimates would be strengthened.\n\n\nc. OLS with malaria\n\nols_malaria &lt;- lm(loggdp ~ risk + malaria, dta)\nols_malaria |&gt;\n  tidy() |&gt;\n  knitr::kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n6.2958652\n0.4087284\n15.403542\n0e+00\n\n\nrisk\n0.3388912\n0.0554135\n6.115683\n1e-07\n\n\nmalaria\n-1.1454612\n0.1825618\n-6.274375\n0e+00\n\n\n\n\n\nWe cannot interpret this causally because the error term of a causal model represents all other causes of loggdp, some of which may also cause institutions or malaria (for instance, geography). Additionally, GDP may also cause good or bad institutions. Simply controlling for malaria would not work for a causal interpretation. Also note that the magnitude on risk on GDP is smaller when controlling for malaria, indicating that malaria may have previously been a factor which explains both risk and loggdp.\n\n\nd. First stage with malaria\n\nfirst_stage_malaria &lt;- lm(risk ~ logmort0 + malaria, dta)\nfirst_stage_malaria |&gt;\n  tidy() |&gt;\n  knitr::kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n8.8342124\n0.7539641\n11.717020\n0.0000000\n\n\nlogmort0\n-0.4380426\n0.1881859\n-2.327712\n0.0233768\n\n\nmalaria\n-0.6962385\n0.5220329\n-1.333706\n0.1874262\n\n\n\n\n\nThis would seem to suggest that a 1 percent increase in mortality rate is associated with .43 point decrease in risk protection. Notice that the p value for malaria is 0.187, which does not pass the 95% test. Intuitively this makes sense if we want to make the case that malaria represents a separate causal path from settler mortality to GDP.\n\n\ne. IV with malaria\n\niv_malaria &lt;- AER::ivreg(loggdp ~ risk + malaria | malaria + logmort0, data = dta)\n\nreg_malaria &lt;- list(ols_malaria = ols_malaria, iv_malaria = iv_malaria, ols = ols, iv = iv)\nmodelsummary(reg_malaria, fmt = 3, gof_omit = 'Log.Lik.|R2 Adj.|AIC|BIC|F')\n\n\n\n\n\nols_malaria\niv_malaria\nols\niv\n\n\n\n\n(Intercept)\n6.296\n4.505\n4.731\n2.135\n\n\n\n(0.409)\n(1.591)\n(0.415)\n(1.014)\n\n\nrisk\n0.339\n0.589\n0.505\n0.905\n\n\n\n(0.055)\n(0.222)\n(0.062)\n(0.155)\n\n\nmalaria\n−1.145\n−0.751\n\n\n\n\n\n(0.183)\n(0.396)\n\n\n\n\nNum.Obs.\n62\n62\n62\n62\n\n\nR2\n0.714\n0.615\n0.523\n0.195\n\n\nRMSE\n0.55\n0.64\n0.71\n0.92\n\n\n\n\n\n\n\nComparing ols_malaria and iv_malaria, we see that the IV estimate is indeed larger than the OLS estimate in magnitude in the effect of good institutions on good economic performance.\n\n\nf.\nThis criticism is valid in that it shows that, controlling for malaria, both the OLS and IV estimates for the effect of institutions have gone down. Therefore it shows that AJR’s estimates are likely overblown. However, in both AJR’s analysis and when we control for malaria, the iv estimates are consistently higher than the OLS estimates, which suggests that the fundamental logic of AJR’s paper, that institutions matter, is still sound, with a caveat that controlling for malaria, the magnitude of this effect goes back to the original OLS level."
  },
  {
    "objectID": "colonial_origins_ex.html#q5-jeffery-sachs-criticism-of-ajr",
    "href": "colonial_origins_ex.html#q5-jeffery-sachs-criticism-of-ajr",
    "title": "The Colonial Origins of Comparative Development",
    "section": "Q5: Jeffery Sachs Criticism of AJR",
    "text": "Q5: Jeffery Sachs Criticism of AJR\n\na.\n\niv_geography &lt;- AER::ivreg(loggdp ~ risk + malaria + latitude + rainmin + meantemp | malaria + latitude + rainmin + meantemp + logmort0, data = dta)\n\nols_geography &lt;- lm(loggdp ~ risk + malaria + latitude + rainmin + meantemp, dta)\n\nreg_malaria_geo &lt;- list(iv_geography = iv_geography, ols_geography = ols_geography, ols_malaria = ols_malaria, iv_malaria = iv_malaria)\n\nmodelsummary(reg_malaria_geo, fmt = 3, gof_omit = 'Log.Lik.|AIC|BIC|F')\n\n\n\n\n\niv_geography\nols_geography\nols_malaria\niv_malaria\n\n\n\n\n(Intercept)\n3.826\n6.956\n6.296\n4.505\n\n\n\n(9.663)\n(0.648)\n(0.409)\n(1.591)\n\n\nrisk\n0.680\n0.258\n0.339\n0.589\n\n\n\n(1.298)\n(0.062)\n(0.055)\n(0.222)\n\n\nmalaria\n−0.746\n−0.998\n−1.145\n−0.751\n\n\n\n(0.818)\n(0.198)\n(0.183)\n(0.396)\n\n\nlatitude\n−0.444\n0.317\n\n\n\n\n\n(2.510)\n(0.678)\n\n\n\n\nrainmin\n−0.001\n0.007\n\n\n\n\n\n(0.025)\n(0.003)\n\n\n\n\nmeantemp\n0.009\n−0.018\n\n\n\n\n\n(0.086)\n(0.017)\n\n\n\n\nNum.Obs.\n62\n62\n62\n62\n\n\nR2\n0.537\n0.745\n0.714\n0.615\n\n\nR2 Adj.\n0.495\n0.722\n0.704\n0.602\n\n\nRMSE\n0.70\n0.52\n0.55\n0.64\n\n\n\n\n\n\n\n\n\nb.\nAfter controlling for the three geographical factors in our IV analysis, we find that nothing in the regression has significant results. However, if we discard settlers mortality as an IV and just run an OLS, none of the geographical explanations are while institutions and malaria are. This may be due to overfitting, whereas the more variables we add the larger that standard deviation would be, or it may say something more important about the instrument. Let’s investigate further.\nOn first sight, there may seem to be a connection between the settlers mortality rates and geography. This would be problematic because this would violate the independence assumption for the instrument logmort0, as Sachs is also claiming that geography is also a factor which GDP depends on. Let’s first run some quick regressions\nTesting if mortality rates and geography are related\n\ngeo_mort &lt;- lm(logmort0 ~ latitude + meantemp + rainmin, dta)\ntidy(geo_mort)|&gt;\n  knitr::kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n3.4115397\n0.7701581\n4.429662\n0.0000424\n\n\nlatitude\n-2.1375517\n1.0764409\n-1.985758\n0.0517940\n\n\nmeantemp\n0.0890122\n0.0275084\n3.235824\n0.0020058\n\n\nrainmin\n-0.0163914\n0.0038366\n-4.272360\n0.0000728\n\n\n\n\n\nTesting if gdp and geography are related\n\ntidy(lm(loggdp ~ latitude + meantemp + rainmin, dta)) |&gt;\n  knitr::kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n8.6603721\n0.6177885\n14.018345\n0.0000000\n\n\nlatitude\n2.0757470\n0.8634757\n2.403944\n0.0194351\n\n\nmeantemp\n-0.0610336\n0.0220661\n-2.765950\n0.0076005\n\n\nrainmin\n0.0158950\n0.0030776\n5.164785\n0.0000031\n\n\n\n\n\nIt seems as though latitude is quite weak, but rainmin and meantemp seems to have a strong correlation with both the instrument and the outcome variables. Stopping to think for a second, as someone who’s experienced both the northern, dry parts of China and southern, humid parts of China, both temperature and rainfall are causes of mosquito infestations, which are the primary medium for malaria, which we use as a proxy for disease today. If we can prove that, controlling for disease, geography itself does not explain settlers mortality, then we can say that we have actually controlled for geography in the malaria control, and would not have to proxy for all the extra variables which are making our standard errors too big. Let’s test for this\n\ngeo_mort_malaria &lt;- lm(logmort0 ~ latitude + meantemp + rainmin + malaria, dta)\n\ngeo_mort_regs &lt;- list(without_malaria = geo_mort, with_malaria = geo_mort_malaria)\n\nmodelsummary(geo_mort_regs, fmt = 3, gof_omit = 'Log.Lik.|AIC|BIC|F')\n\n\n\n\n\nwithout_malaria\nwith_malaria\n\n\n\n\n(Intercept)\n3.412\n3.151\n\n\n\n(0.770)\n(0.648)\n\n\nlatitude\n−2.138\n−0.565\n\n\n\n(1.076)\n(0.955)\n\n\nmeantemp\n0.089\n0.057\n\n\n\n(0.028)\n(0.024)\n\n\nrainmin\n−0.016\n−0.011\n\n\n\n(0.004)\n(0.003)\n\n\nmalaria\n\n1.401\n\n\n\n\n(0.277)\n\n\nNum.Obs.\n62\n62\n\n\nR2\n0.471\n0.635\n\n\nR2 Adj.\n0.444\n0.609\n\n\nRMSE\n0.90\n0.75\n\n\n\n\n\n\n\nAs we can see, when we control for malaria, even though the three geography controls are still significant to some extent, none of them have any significant impact on explaining settlers mortality compared to before (especially with the latitude factor). We also see a jump in adj. R squared. Even though these results does not completely prove my point (meantemp and rainmin still have very small yet significant impact on settlers mortality controlling for malaria), it does call in to question the need to control for three new variables in the IV regression, which increases the standard error of each coefficient estiamte significantly and does not contribute massively to explaining the variance if most is already captured in the malaria variable (which we also controlled for in the IV regression)."
  },
  {
    "objectID": "contaminated_wells_ex.html",
    "href": "contaminated_wells_ex.html",
    "title": "Comtaminated Wells in Bangladesh",
    "section": "",
    "text": "data_url &lt;- 'https://ditraglia.com/data/wells.csv'\nwells &lt;- read_csv(data_url)\n\nRows: 3020 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (5): switch, arsenic, dist, assoc, educ\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(wells)\n\n# A tibble: 6 × 5\n  switch arsenic  dist assoc  educ\n   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1      1    2.36  16.8     0     0\n2      1    0.71  47.3     0     0\n3      0    2.07  21.0     0    10\n4      1    1.15  21.5     0    12\n5      1    1.1   40.9     1    14\n6      1    3.9   69.5     1     9\n\n\n\n\n\n\nwells &lt;- wells |&gt;\n  mutate(larsenic = log(arsenic))\n\n\n\n\n\np1 &lt;- wells |&gt;\n  mutate(larsenic = log(arsenic)) |&gt;\n  ggplot(aes(x = arsenic)) +\n  geom_histogram()\n\np2 &lt;- wells |&gt;\n  mutate(larsenic = log(arsenic)) |&gt;\n  ggplot(aes(x = larsenic)) +\n  geom_histogram()\n\ngrid.arrange(p1, p2, ncol = 2)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nMost households have arsenic levels close to 0, and very few have levels greater than 5. Therefore when we take the log of these values most will fall between negative infinity (those that are close to 0) to 1 (approx.), below is a natural log plot to better visualize this relationship\n\nx &lt;- seq(0, 10, by = 0.01)\ny &lt;- log(x)\ndata.frame(x,y) |&gt;\n  ggplot(aes(x = x, y = y)) +\n  geom_line() +\n  geom_abline(intercept = log(2.5), slope = 0, color = \"blue\")\n\n\n\n\n\n\n\n\nwells &lt;- wells |&gt;\n  mutate(dist100 = dist/100)\n\n\n\n\n\nwells &lt;- wells |&gt;\n  mutate(zeduc = (educ - mean(educ)) / sd(educ) )"
  },
  {
    "objectID": "contaminated_wells_ex.html#q1-preliminarites",
    "href": "contaminated_wells_ex.html#q1-preliminarites",
    "title": "Comtaminated Wells in Bangladesh",
    "section": "",
    "text": "data_url &lt;- 'https://ditraglia.com/data/wells.csv'\nwells &lt;- read_csv(data_url)\n\nRows: 3020 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (5): switch, arsenic, dist, assoc, educ\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(wells)\n\n# A tibble: 6 × 5\n  switch arsenic  dist assoc  educ\n   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1      1    2.36  16.8     0     0\n2      1    0.71  47.3     0     0\n3      0    2.07  21.0     0    10\n4      1    1.15  21.5     0    12\n5      1    1.1   40.9     1    14\n6      1    3.9   69.5     1     9\n\n\n\n\n\n\nwells &lt;- wells |&gt;\n  mutate(larsenic = log(arsenic))\n\n\n\n\n\np1 &lt;- wells |&gt;\n  mutate(larsenic = log(arsenic)) |&gt;\n  ggplot(aes(x = arsenic)) +\n  geom_histogram()\n\np2 &lt;- wells |&gt;\n  mutate(larsenic = log(arsenic)) |&gt;\n  ggplot(aes(x = larsenic)) +\n  geom_histogram()\n\ngrid.arrange(p1, p2, ncol = 2)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nMost households have arsenic levels close to 0, and very few have levels greater than 5. Therefore when we take the log of these values most will fall between negative infinity (those that are close to 0) to 1 (approx.), below is a natural log plot to better visualize this relationship\n\nx &lt;- seq(0, 10, by = 0.01)\ny &lt;- log(x)\ndata.frame(x,y) |&gt;\n  ggplot(aes(x = x, y = y)) +\n  geom_line() +\n  geom_abline(intercept = log(2.5), slope = 0, color = \"blue\")\n\n\n\n\n\n\n\n\nwells &lt;- wells |&gt;\n  mutate(dist100 = dist/100)\n\n\n\n\n\nwells &lt;- wells |&gt;\n  mutate(zeduc = (educ - mean(educ)) / sd(educ) )"
  },
  {
    "objectID": "contaminated_wells_ex.html#q2-first-regression-fit1",
    "href": "contaminated_wells_ex.html#q2-first-regression-fit1",
    "title": "Comtaminated Wells in Bangladesh",
    "section": "Q2: First Regression: fit1",
    "text": "Q2: First Regression: fit1\n\na)\n\nfit1 &lt;- glm(switch ~ dist100, family = binomial(link = 'logit'), wells)\nmodelsummary(fit1)\n\n\n\n\n\n (1)\n\n\n\n\n(Intercept)\n0.606\n\n\n\n(0.060)\n\n\ndist100\n−0.622\n\n\n\n(0.097)\n\n\nNum.Obs.\n3020\n\n\nAIC\n4080.2\n\n\nBIC\n4092.3\n\n\nLog.Lik.\n−2038.119\n\n\nF\n40.744\n\n\nRMSE\n0.49\n\n\n\n\n\n\n\n\n\nb)\n\nwells |&gt;\n  ggplot(aes(x = dist100, y = switch)) +\n  stat_smooth(method = 'glm', method.args = list(family = \"binomial\"), formula = y ~ x) +\n  geom_jitter(width = 0.1, height = 0.01) + \n  xlab(\"distance to nearest clean well / 100 m\") +\n  ylab(\"predicted probability for switching\")\n\n\n\n\n\n\nc)\nFrom the first two parts we see that there is a small but significant relationship between the distance (a 100m increase in distance to a clean well will decrease the log odds by approx. 0.6). This would make sense since we would assume that, given that the households know if their nearest well is contaminated or not, that they would be less inclined to move if the nearest clean well is far.\n\n\nd)\n\nmean_dist &lt;- wells |&gt;\n  select(dist100) |&gt;\n  summarize(dist100 = mean(dist100))\n\nprob_avg &lt;- predict(fit1, mean_dist, type = 'response')\nprob_avg\n\n        1 \n0.5757602 \n\n\nFor the average household (in terms of distance to the nearest safe well), the probability of switching is approx .57\n\n\ne)\nFirst let us calculate the marginal effect from taking the derivative of \\(P(Y = 1 | X)\\)\n[ \\[\\begin{align}\n\\frac{\\partial p(x)}{\\partial x} &= \\beta_1 \\frac{e^{\\beta_0 + \\beta_1 X}}{(1 + e^{\\beta_0 + \\beta_1 X})^2} \\\\\n&= \\beta_1 \\frac{e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}} \\frac{1}{1 + e^{\\beta_0 + \\beta_1 X}} \\\\\n& = \\beta_1 P(Y = 1) (1-P(Y = 1))\n\\end{align}\\] ]\nNow we calcualte this: by using the result from the previous question prob_avg which represent \\(P(Y = 1)\\):\n\nmarginal_avg &lt;- as.numeric(coef(fit1)[2] * prob_avg * (1 - prob_avg))\nmarginal_avg\n\n[1] -0.1519011\n\n\nThis figure suggests that for the person which corresponds to the average distance towards the nearest clean well, the regression would predict that if they live 100 meters further, we would predict a 0.15 decrease in the probability of moving. Now let’s find the “divide by 4” and average partial effect\n\ndiv4 &lt;- as.numeric(coef(fit1)[2]/4)\n\navg_partial &lt;- mean(coef(fit1)[2] * predict(fit1, type = 'response') * (1 - predict(fit1, type = 'response')))\n\ndata.frame(divide_by_4 = div4, avg_partial, marginal_at_mean = marginal_avg)\n\n  divide_by_4 avg_partial marginal_at_mean\n1  -0.1554705   -0.149842       -0.1519011\n\n\nThe marginal effect of the average household is sandwiched between the average partial effect and the “divide by 4” value. This suggests that the effect of the independent variable on the probability of the event occurring at the average values of the predictors is less than the maximum possible effect (as given by the “divide by 4” rule). Additionally, on average, the effect of the independent variable on the probability of the event occurring is less than its effect at the average values of the predictors."
  },
  {
    "objectID": "contaminated_wells_ex.html#q3-predictive-performance-of-fit1",
    "href": "contaminated_wells_ex.html#q3-predictive-performance-of-fit1",
    "title": "Comtaminated Wells in Bangladesh",
    "section": "Q3: Predictive performance of fit1",
    "text": "Q3: Predictive performance of fit1\n\na)\n\nwells &lt;- wells |&gt;\n  mutate(p1 = predict(fit1, type = 'response')) |&gt;\n  select(p1, dist100, everything())\n\n\n\nb)\n\nwells &lt;- wells |&gt; \n  mutate(pred1 = ifelse(p1 &gt; 0.5, 1, 0)) |&gt;\n  select(pred1, everything())\n\nI am going to assume to use the bayes classifer rule for constructing the pred1 ## c)\n\nerror_fit1 &lt;- wells |&gt; \n  summarize(error_rate = mean(pred1 - switch))\nerror_fit1\n\n# A tibble: 1 × 1\n  error_rate\n       &lt;dbl&gt;\n1      0.317\n\n\nThis means that pred1 overestimates the switch rate for approx. 30 percent of the data.\n\n\nd)\n\ntrue_positive &lt;- sum(as.numeric(wells$pred1 == 1 & wells$switch == 1))\ntrue_negative &lt;- sum(as.numeric(wells$pred1 == 0 & wells$switch == 0))\nfalse_positive &lt;- sum(as.numeric(wells$pred1 == 1 & wells$switch == 0))\n# your website has a typo when explaining what this one was\nfalse_negative &lt;- sum(as.numeric(wells$pred1 == 0 & wells$switch == 1))\nconf_mat &lt;- table(1:2, 1:2)\nconf_mat[1,1] &lt;- true_negative\nconf_mat[2,2] &lt;- true_positive\nconf_mat[1,2] &lt;- false_negative\nconf_mat[2,1] &lt;- false_positive\nconf_mat\n\n   \n       1    2\n  1  194  133\n  2 1089 1604\n\n# I later found out that there is an easier way to do this... \nconf_mat1 &lt;- table(wells$pred1, wells$switch)\n\n\n\ne)\n\nsens_fit1 &lt;- conf_mat1[2,2] / (conf_mat1[2,2] + conf_mat1[1,2])\nspes_fit1 &lt;- conf_mat1[1,1] / (conf_mat1[1,1] + conf_mat1[2,1])\ndata.frame(cbind(sens_fit1, spes_fit1))\n\n  sens_fit1 spes_fit1\n1 0.9234312 0.1512081\n\n\n\n\nf)\nThe results in the previous part makes intuitive sense because we found in part c) that pred1 overestimates the switch rate for approx. 30 percent of the data. This means that pred1 should systematically pick up true values well but false values poorly.\n\nwells |&gt;\n  summarize(mean(switch))\n\n# A tibble: 1 × 1\n  `mean(switch)`\n           &lt;dbl&gt;\n1          0.575\n\n\nThis would mean that the null model predicts a 100 percent switch rate, let’s use this to calculate the error rate (we don’t need to calcualte the sensitivity and specificity because they would be 100 percnet and 0 percent respectively)\n\nwells |&gt;\n  summarize(error_rate_null = 1 - mean(switch))\n\n# A tibble: 1 × 1\n  error_rate_null\n            &lt;dbl&gt;\n1           0.425\n\n\nThe null model overclassifies approx. 42 percent of the data, which means that the null model performs worse than fit1 (or in other words, including dist100 is better at predicting the outcome of switch than not including it)."
  },
  {
    "objectID": "contaminated_wells_ex.html#q4-additional-regressions-fit2-fit3-and-fit4",
    "href": "contaminated_wells_ex.html#q4-additional-regressions-fit2-fit3-and-fit4",
    "title": "Comtaminated Wells in Bangladesh",
    "section": "Q4: Additional regressions: fit2, fit3, and fit4",
    "text": "Q4: Additional regressions: fit2, fit3, and fit4\n\na)\n\nfit2 &lt;- glm(switch ~ larsenic, family = binomial(link='logit'), wells)\n\n\n\nb)\n\nfit3 &lt;- glm(switch ~ zeduc, family = binomial(link = 'logit'), wells)\n\n\n\nc)\n\nfit4 &lt;- glm(switch ~ dist100 + larsenic + zeduc, family = binomial(link = 'logit'), wells)\n\n\n\nd)\n\nwell_regs &lt;- list(fit1, fit2, fit3, fit4)\nmodelsummary(well_regs, fmt = 2, \n             title = \"Regression results for wells dataset\")\n\n\nRegression results for wells dataset\n\n\n\n (1)\n  (2)\n  (3)\n  (4)\n\n\n\n\n(Intercept)\n0.61\n0.10\n0.30\n0.53\n\n\n\n(0.06)\n(0.04)\n(0.04)\n(0.06)\n\n\ndist100\n−0.62\n\n\n−0.98\n\n\n\n(0.10)\n\n\n(0.11)\n\n\nlarsenic\n\n0.71\n\n0.89\n\n\n\n\n(0.06)\n\n(0.07)\n\n\nzeduc\n\n\n0.16\n0.17\n\n\n\n\n\n(0.04)\n(0.04)\n\n\nNum.Obs.\n3020\n3020\n3020\n3020\n\n\nAIC\n4080.2\n3993.3\n4104.4\n3886.2\n\n\nBIC\n4092.3\n4005.3\n4116.4\n3910.2\n\n\nLog.Lik.\n−2038.119\n−1994.644\n−2050.193\n−1939.077\n\n\nF\n40.744\n122.106\n17.531\n70.581\n\n\nRMSE\n0.49\n0.48\n0.49\n0.47"
  },
  {
    "objectID": "contaminated_wells_ex.html#q5-interpreting-fit2-fit3-fit4",
    "href": "contaminated_wells_ex.html#q5-interpreting-fit2-fit3-fit4",
    "title": "Comtaminated Wells in Bangladesh",
    "section": "Q5: Interpreting fit2, fit3, fit4",
    "text": "Q5: Interpreting fit2, fit3, fit4\n\na)\n\nwells |&gt;\n  ggplot(aes(x = larsenic, y = switch)) +\n  geom_smooth(method = 'glm', method.args = list(family = \"binomial\")) +\n  geom_jitter(width = 0.5, height = 0.07)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThere is a statistically significant and positive relationship between the log arsenic levels and switch rate. The sign makes intuitive sense because the higher the arsenic levels, the odds of switching should increase.\n\n\nb)\n\nwells |&gt;\n  ggplot(aes(x = zeduc, y = switch)) +\n  geom_smooth(method = \"glm\", method.args = list(family = \"binomial\"))+\n  geom_jitter(width = 0.5, height = 0.1)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nIt seems as if even though there is a statistically significant relationship between the the change of 1 standard deviation of education and the log odds of switching, although the magnitude of this effect is quite small. The sign of its coefficient is positive which may be because: people who have had more education on average is more aware of the long-term negative health effects of contaminated wells. However, it is good to note that this is an average effect and there are many people in the dataset who have 1 standard deviation less than the average education, who do switch to a healthier well.\n\n\nc)\n\nfit4_mean &lt;- wells |&gt;\n  select(dist100, larsenic, zeduc) |&gt;\n  summarize(dist100_mean = mean(dist100), \n            larsenic_mean = mean(larsenic), \n            zeduc_mean = mean(zeduc)) |&gt;\n  mutate(predic_mean = predict(fit4, data.frame(dist100 = dist100_mean, larsenic = larsenic_mean, zeduc = zeduc_mean), type = \"response\"))\n\nmarg_dist &lt;- coef(fit4)[2] * fit4_mean$predic_mean * (1 - fit4_mean$predic_mean)\nmarg_larsenic &lt;- coef(fit4)[3] * fit4_mean$predic_mean * (1 - fit4_mean$predic_mean)\nmarg_zeduc &lt;- coef(fit4)[4] * fit4_mean$predic_mean * (1 - fit4_mean$predic_mean)\nmarg_fit4 &lt;- cbind(marg_dist, marg_larsenic, marg_zeduc)\nrownames(marg_fit4) &lt;- \"marg_effect_fit4\"\n\ndiv_dist &lt;- coef(fit4)[2] / 4\ndiv_larsenic &lt;- coef(fit4)[3] / 4\ndiv_zeduc &lt;- coef(fit4)[4] / 4\ndiv4_fit4 &lt;- cbind(div_dist, div_larsenic, div_zeduc)\nrownames(div4_fit4) &lt;- \"div4_fit4\"\n\ndata.frame(rbind(marg_fit4, div4_fit4))\n\n                 marg_dist marg_larsenic marg_zeduc\nmarg_effect_fit4 -0.238147     0.2162507 0.04212322\ndiv4_fit4        -0.244733     0.2222313 0.04328816\n\n\nIt seems that distance to the well has a slightly larger impact on the probability of switching than the log arsenic levels for someone who has the mean value of all three predictors. The level of education (number of sd from the mean) compared to the other two predictors has minimal impact on the probability of switching, according to this regression. The divide by four rules gives larger marginal effective (by magnitude) than the three marginal effects for the average household."
  },
  {
    "objectID": "contaminated_wells_ex.html#q6-predictive-performance-of-fit4",
    "href": "contaminated_wells_ex.html#q6-predictive-performance-of-fit4",
    "title": "Comtaminated Wells in Bangladesh",
    "section": "Q6: Predictive performance of fit4",
    "text": "Q6: Predictive performance of fit4\n\na)\n\na)\n\nwells &lt;- wells |&gt;\n  mutate(p4 = predict(fit4, type = 'response')) |&gt;\n  select(p4, dist100, everything())\n\n\n\nb)\n\nwells &lt;- wells |&gt; \n  mutate(pred4 = ifelse(p4 &gt; 0.5, 1, 0)) |&gt;\n  select(pred4, everything())\n\nI am going to assume to use the bayes classifer rule for constructing the pred1 ### c)\n\nerror_fit4 &lt;- wells |&gt; \n  summarize(error_rate = mean(pred4 - switch))\nerror_fit4\n\n# A tibble: 1 × 1\n  error_rate\n       &lt;dbl&gt;\n1      0.119\n\n\nThis means that pred1 overestimates the switch rate for approx. 30 percent of the data.\n\n\nd)\n\nconf_mat2 &lt;- table(wells$pred4, wells$switch)\nconf_mat2\n\n   \n       0    1\n  0  546  379\n  1  737 1358\n\n\n\n\ne)\n\nsens_fit4 &lt;- conf_mat2[2,2] / (conf_mat2[2,2] + conf_mat2[1,2])\nspes_fit4 &lt;- conf_mat2[1,1] / (conf_mat2[1,1] + conf_mat2[2,1])\ndata.frame(cbind(sens_fit4, spes_fit4))\n\n  sens_fit4 spes_fit4\n1 0.7818077 0.4255651\n\n\n\n\nf)\n\nwells |&gt;\n  summarize(mean(switch))\n\n# A tibble: 1 × 1\n  `mean(switch)`\n           &lt;dbl&gt;\n1          0.575\n\n\nThis regression has a significant reduction or error compared to the null model, as compared to the null model of everyone switching it has much better spesificity.\n\n\n\nb)\n\nfit4_performance &lt;- data.frame(cbind(sens = sens_fit4, spes = spes_fit4, error_rate = error_fit4))\nrownames(fit4_performance) &lt;- \"fit4_perf\"\n\nfit1_performance &lt;- data.frame(cbind(sens = sens_fit1, spes = spes_fit1, error_rate = error_fit1))\nrownames(fit1_performance) &lt;- \"fit1_perf\"\nrbind(fit1_performance, fit4_performance)\n\n               sens      spes error_rate\nfit1_perf 0.9234312 0.1512081  0.3165563\nfit4_perf 0.7818077 0.4255651  0.1185430\n\n\nAs we can see form this tibble, fit1 has very high sensitivity with very low specificity, while fit4 has lower sensitivity but much better specificity and a lower error rate. This suggests that fit4 is better than fit1 at correctly predicting cases where a household does not switch to a clean well. This improvement also outweighs its lack of predictive power when it comes to correctly predicting a household switching when it actually does switch, as it has a overall lower error rate (which both sensitivity and specificity contributes towards)."
  }
]